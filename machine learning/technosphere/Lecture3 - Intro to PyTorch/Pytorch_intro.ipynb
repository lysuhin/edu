{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 3: Библиотеки для глубинного обучения. Примитивы фремворка Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#        Пример обучения нейронной сети в numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1488116441.429096"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 31118960.634845525)\n",
      "(1, 28986309.007656906)\n",
      "(2, 32625859.296431243)\n",
      "(3, 36534370.557906404)\n",
      "(4, 35436174.930721447)\n",
      "(5, 26980874.653306477)\n",
      "(6, 16038472.455092344)\n",
      "(7, 7921745.7438624324)\n",
      "(8, 3809572.4481635652)\n",
      "(9, 2033960.1744381748)\n",
      "(10, 1283732.6907986884)\n",
      "(11, 930708.8634522853)\n",
      "(12, 734286.18606640119)\n",
      "(13, 605394.9199803808)\n",
      "(14, 510348.44950401766)\n",
      "(15, 435637.54003521544)\n",
      "(16, 374827.26173134742)\n",
      "(17, 324387.58175882266)\n",
      "(18, 282051.92592800688)\n",
      "(19, 246272.17142690078)\n",
      "(20, 215794.79786549602)\n",
      "(21, 189684.88588497846)\n",
      "(22, 167218.34695468962)\n",
      "(23, 147832.68255369243)\n",
      "(24, 131018.00410826772)\n",
      "(25, 116395.72645295241)\n",
      "(26, 103633.27030852422)\n",
      "(27, 92461.740984773118)\n",
      "(28, 82661.936509389867)\n",
      "(29, 74049.133226521313)\n",
      "(30, 66457.769516926463)\n",
      "(31, 59744.935377604648)\n",
      "(32, 53793.332395702848)\n",
      "(33, 48520.30119766909)\n",
      "(34, 43834.296961918793)\n",
      "(35, 39655.916475319129)\n",
      "(36, 35921.658141018401)\n",
      "(37, 32578.407048062916)\n",
      "(38, 29580.676624200816)\n",
      "(39, 26889.779220617831)\n",
      "(40, 24471.890732239488)\n",
      "(41, 22295.760562333249)\n",
      "(42, 20335.466932146373)\n",
      "(43, 18565.157614750475)\n",
      "(44, 16963.708643929815)\n",
      "(45, 15513.374207186538)\n",
      "(46, 14198.484956074262)\n",
      "(47, 13005.188243319388)\n",
      "(48, 11921.666617589086)\n",
      "(49, 10936.445631131541)\n",
      "(50, 10038.865165574516)\n",
      "(51, 9221.9970658047732)\n",
      "(52, 8477.6971414899253)\n",
      "(53, 7798.576720257136)\n",
      "(54, 7178.0787894570067)\n",
      "(55, 6610.7674599920974)\n",
      "(56, 6091.6925906940251)\n",
      "(57, 5616.4107452747303)\n",
      "(58, 5180.9918023203554)\n",
      "(59, 4781.5382808337527)\n",
      "(60, 4415.1144049560071)\n",
      "(61, 4078.5413392368537)\n",
      "(62, 3769.3027132309217)\n",
      "(63, 3485.1770830621226)\n",
      "(64, 3223.6129281143662)\n",
      "(65, 2982.8860518975825)\n",
      "(66, 2761.6441663008763)\n",
      "(67, 2558.0471909064318)\n",
      "(68, 2370.2994252296121)\n",
      "(69, 2197.1147847133625)\n",
      "(70, 2037.2271431211709)\n",
      "(71, 1889.6904160730899)\n",
      "(72, 1753.4112286674613)\n",
      "(73, 1627.4657858778683)\n",
      "(74, 1511.0739913541379)\n",
      "(75, 1403.3667836085342)\n",
      "(76, 1303.7798536532905)\n",
      "(77, 1211.5549894555102)\n",
      "(78, 1126.1865838988515)\n",
      "(79, 1047.1108407244317)\n",
      "(80, 973.87695967030174)\n",
      "(81, 906.01445911896781)\n",
      "(82, 843.04101343617413)\n",
      "(83, 784.63381129908635)\n",
      "(84, 730.45363713995891)\n",
      "(85, 680.19903786051839)\n",
      "(86, 633.51795101981838)\n",
      "(87, 590.17679386958434)\n",
      "(88, 549.91457917338232)\n",
      "(89, 512.51570322228599)\n",
      "(90, 477.75191759913844)\n",
      "(91, 445.42879923689827)\n",
      "(92, 415.38142820735129)\n",
      "(93, 387.43187111545382)\n",
      "(94, 361.44927586250776)\n",
      "(95, 337.24831120409726)\n",
      "(96, 314.72905582762019)\n",
      "(97, 293.76382943689293)\n",
      "(98, 274.24731284769945)\n",
      "(99, 256.07257147970518)\n",
      "(100, 239.1349346798425)\n",
      "(101, 223.35930462998664)\n",
      "(102, 208.65923865914763)\n",
      "(103, 194.96360210007825)\n",
      "(104, 182.19037116820064)\n",
      "(105, 170.2793658657265)\n",
      "(106, 159.18067928229522)\n",
      "(107, 148.83282012745394)\n",
      "(108, 139.17938997097713)\n",
      "(109, 130.16490413991502)\n",
      "(110, 121.75508626090519)\n",
      "(111, 113.90172606872397)\n",
      "(112, 106.57459913836107)\n",
      "(113, 99.727874020513951)\n",
      "(114, 93.333673980848957)\n",
      "(115, 87.363074901531476)\n",
      "(116, 81.783652205832098)\n",
      "(117, 76.571768860277075)\n",
      "(118, 71.699422031985804)\n",
      "(119, 67.14435835583761)\n",
      "(120, 62.886294487773185)\n",
      "(121, 58.908956927674353)\n",
      "(122, 55.187980867551801)\n",
      "(123, 51.706285625842533)\n",
      "(124, 48.450577129499784)\n",
      "(125, 45.405405099125026)\n",
      "(126, 42.557883901301949)\n",
      "(127, 39.891359529795238)\n",
      "(128, 37.395894614671377)\n",
      "(129, 35.060879495861656)\n",
      "(130, 32.876279177756274)\n",
      "(131, 30.830152346097734)\n",
      "(132, 28.913985652408709)\n",
      "(133, 27.119467848685051)\n",
      "(134, 25.439128495429138)\n",
      "(135, 23.866159937425895)\n",
      "(136, 22.391971325428422)\n",
      "(137, 21.010507449842869)\n",
      "(138, 19.716571474368394)\n",
      "(139, 18.504319173238045)\n",
      "(140, 17.368397693945742)\n",
      "(141, 16.303114555342432)\n",
      "(142, 15.304806469907328)\n",
      "(143, 14.369304668469415)\n",
      "(144, 13.492168148173437)\n",
      "(145, 12.669561664130624)\n",
      "(146, 11.898030334361151)\n",
      "(147, 11.174480108585183)\n",
      "(148, 10.495702425823406)\n",
      "(149, 9.8594622031280181)\n",
      "(150, 9.2622269982130163)\n",
      "(151, 8.7017779467087877)\n",
      "(152, 8.1759712181890105)\n",
      "(153, 7.6826507744454009)\n",
      "(154, 7.2199136453810748)\n",
      "(155, 6.7853583790899474)\n",
      "(156, 6.3773981420899526)\n",
      "(157, 5.9945230176367144)\n",
      "(158, 5.6351221063772474)\n",
      "(159, 5.2976922588106934)\n",
      "(160, 4.9806947754612754)\n",
      "(161, 4.6830865252491876)\n",
      "(162, 4.4036349145020193)\n",
      "(163, 4.1412456484719442)\n",
      "(164, 3.8948103859522862)\n",
      "(165, 3.663129448720968)\n",
      "(166, 3.4454605429072229)\n",
      "(167, 3.2409735040929903)\n",
      "(168, 3.049024572596915)\n",
      "(169, 2.8688904555957753)\n",
      "(170, 2.699637198041223)\n",
      "(171, 2.5405367037966862)\n",
      "(172, 2.3909336754429686)\n",
      "(173, 2.2503889927248526)\n",
      "(174, 2.1181574944179484)\n",
      "(175, 1.99382530586889)\n",
      "(176, 1.8769050505450391)\n",
      "(177, 1.7670098046077005)\n",
      "(178, 1.6637212894976856)\n",
      "(179, 1.5664914043451643)\n",
      "(180, 1.4750230649438612)\n",
      "(181, 1.3890110876400517)\n",
      "(182, 1.3081184111624393)\n",
      "(183, 1.232009993557017)\n",
      "(184, 1.1603591760786596)\n",
      "(185, 1.0929618545743987)\n",
      "(186, 1.0295109353687399)\n",
      "(187, 0.96981029514807549)\n",
      "(188, 0.91365972049546629)\n",
      "(189, 0.86078350260121617)\n",
      "(190, 0.81099454061864984)\n",
      "(191, 0.76414059218743713)\n",
      "(192, 0.72005033732600121)\n",
      "(193, 0.67856059133147317)\n",
      "(194, 0.63945673172762185)\n",
      "(195, 0.60264239267539976)\n",
      "(196, 0.56797376056658222)\n",
      "(197, 0.53533588210989591)\n",
      "(198, 0.50462850284996519)\n",
      "(199, 0.4756866343948255)\n",
      "(200, 0.44842137235940427)\n",
      "(201, 0.4227371762176414)\n",
      "(202, 0.39854701135276094)\n",
      "(203, 0.37578149751527778)\n",
      "(204, 0.35431567121851448)\n",
      "(205, 0.33408710703536348)\n",
      "(206, 0.31502747182810303)\n",
      "(207, 0.29707648242251983)\n",
      "(208, 0.28016928975811273)\n",
      "(209, 0.26423100556723678)\n",
      "(210, 0.24920998577575082)\n",
      "(211, 0.23505176815225765)\n",
      "(212, 0.22170514023857257)\n",
      "(213, 0.20913525788135703)\n",
      "(214, 0.19728339633067232)\n",
      "(215, 0.1861074477466729)\n",
      "(216, 0.17557409542790703)\n",
      "(217, 0.16564665058082084)\n",
      "(218, 0.15629114028298924)\n",
      "(219, 0.14746639968004849)\n",
      "(220, 0.13914316042795166)\n",
      "(221, 0.13129690567453145)\n",
      "(222, 0.12389596632993166)\n",
      "(223, 0.11691984389280588)\n",
      "(224, 0.11034372345734927)\n",
      "(225, 0.10413679430608011)\n",
      "(226, 0.098284033733097215)\n",
      "(227, 0.092763789002275776)\n",
      "(228, 0.087558261697329179)\n",
      "(229, 0.082649954974009718)\n",
      "(230, 0.078017984957803696)\n",
      "(231, 0.073647745658003541)\n",
      "(232, 0.069523773757778323)\n",
      "(233, 0.065632812772235177)\n",
      "(234, 0.061964852270206858)\n",
      "(235, 0.058503481802748447)\n",
      "(236, 0.055236077753771651)\n",
      "(237, 0.052153059266076607)\n",
      "(238, 0.049240310518008104)\n",
      "(239, 0.046493105270167125)\n",
      "(240, 0.043901160972482138)\n",
      "(241, 0.041454566325367793)\n",
      "(242, 0.039146013690185694)\n",
      "(243, 0.036966522241563271)\n",
      "(244, 0.034909396391714018)\n",
      "(245, 0.03296968244446577)\n",
      "(246, 0.031137513495952555)\n",
      "(247, 0.029408590039254348)\n",
      "(248, 0.027776154455119925)\n",
      "(249, 0.026235571987082637)\n",
      "(250, 0.024781482986616005)\n",
      "(251, 0.023408645326790233)\n",
      "(252, 0.0221122238933011)\n",
      "(253, 0.020888468675533588)\n",
      "(254, 0.01973267974068605)\n",
      "(255, 0.018641905187154757)\n",
      "(256, 0.017612789543712138)\n",
      "(257, 0.016639714559638523)\n",
      "(258, 0.015720815341388077)\n",
      "(259, 0.014853203887136967)\n",
      "(260, 0.014034091621439083)\n",
      "(261, 0.013260782161631061)\n",
      "(262, 0.01253054804981964)\n",
      "(263, 0.01184074187688154)\n",
      "(264, 0.011188869411107607)\n",
      "(265, 0.010573220616871843)\n",
      "(266, 0.0099918464577489624)\n",
      "(267, 0.0094431294192881855)\n",
      "(268, 0.0089243011016393974)\n",
      "(269, 0.0084342741709521312)\n",
      "(270, 0.0079713274481721004)\n",
      "(271, 0.0075340465733897237)\n",
      "(272, 0.0071210857497376618)\n",
      "(273, 0.0067309999814472877)\n",
      "(274, 0.0063624639125358443)\n",
      "(275, 0.0060141022640353252)\n",
      "(276, 0.0056850080293945997)\n",
      "(277, 0.0053739510405009959)\n",
      "(278, 0.0050802036886104076)\n",
      "(279, 0.0048025667226624779)\n",
      "(280, 0.0045401557831758355)\n",
      "(281, 0.0042922357354662748)\n",
      "(282, 0.0040578835889090803)\n",
      "(283, 0.0038364892057114433)\n",
      "(284, 0.0036273746718903021)\n",
      "(285, 0.0034296148569028838)\n",
      "(286, 0.0032426995332593372)\n",
      "(287, 0.0030660472314386501)\n",
      "(288, 0.0028990663160171032)\n",
      "(289, 0.0027413228730893144)\n",
      "(290, 0.0025922875237075643)\n",
      "(291, 0.0024512630774180134)\n",
      "(292, 0.0023179663262916515)\n",
      "(293, 0.002191954602664528)\n",
      "(294, 0.0020728514231377433)\n",
      "(295, 0.0019603182479782282)\n",
      "(296, 0.0018539942019757418)\n",
      "(297, 0.00175339371745848)\n",
      "(298, 0.0016582541455416101)\n",
      "(299, 0.0015683572371956236)\n",
      "(300, 0.0014833448552815636)\n",
      "(301, 0.001402996416469319)\n",
      "(302, 0.0013270558618151285)\n",
      "(303, 0.0012551964846170033)\n",
      "(304, 0.0011872431940671891)\n",
      "(305, 0.0011229924127541782)\n",
      "(306, 0.0010622354664232357)\n",
      "(307, 0.001004818116116558)\n",
      "(308, 0.00095052440927727473)\n",
      "(309, 0.0008991877188070927)\n",
      "(310, 0.00085062181700514179)\n",
      "(311, 0.00080468810067984274)\n",
      "(312, 0.00076124063444027944)\n",
      "(313, 0.00072018385837878699)\n",
      "(314, 0.000681333815919721)\n",
      "(315, 0.00064458917513730526)\n",
      "(316, 0.00060985145533861817)\n",
      "(317, 0.00057698424570915463)\n",
      "(318, 0.00054589011879708962)\n",
      "(319, 0.00051650739499396845)\n",
      "(320, 0.0004887004596484622)\n",
      "(321, 0.00046240611240322432)\n",
      "(322, 0.00043753754250226907)\n",
      "(323, 0.00041399811130808433)\n",
      "(324, 0.00039172889950759692)\n",
      "(325, 0.00037068569588764768)\n",
      "(326, 0.00035076403909799767)\n",
      "(327, 0.00033191560205664494)\n",
      "(328, 0.00031408461797109287)\n",
      "(329, 0.000297223962420637)\n",
      "(330, 0.000281266619357882)\n",
      "(331, 0.00026618603670259801)\n",
      "(332, 0.00025190677897764355)\n",
      "(333, 0.00023839605701372836)\n",
      "(334, 0.0002256106518365907)\n",
      "(335, 0.00021351762120069693)\n",
      "(336, 0.00020207585878155354)\n",
      "(337, 0.00019125855177246258)\n",
      "(338, 0.00018101303311616709)\n",
      "(339, 0.00017131814475303682)\n",
      "(340, 0.00016214529675097499)\n",
      "(341, 0.00015346785414084246)\n",
      "(342, 0.00014526052943083438)\n",
      "(343, 0.00013750158020255754)\n",
      "(344, 0.00013015122424629217)\n",
      "(345, 0.00012319358423931379)\n",
      "(346, 0.00011660843462386251)\n",
      "(347, 0.00011037640551441761)\n",
      "(348, 0.00010448077661157297)\n",
      "(349, 9.8905712491261915e-05)\n",
      "(350, 9.36248665443917e-05)\n",
      "(351, 8.8626706692057698e-05)\n",
      "(352, 8.3896214020294201e-05)\n",
      "(353, 7.941892188851994e-05)\n",
      "(354, 7.5182073597672132e-05)\n",
      "(355, 7.1176246389770278e-05)\n",
      "(356, 6.7383718120080547e-05)\n",
      "(357, 6.3791952149278041e-05)\n",
      "(358, 6.0392632462700769e-05)\n",
      "(359, 5.7174685193732013e-05)\n",
      "(360, 5.4128651357402023e-05)\n",
      "(361, 5.1248323213805514e-05)\n",
      "(362, 4.8521232945628117e-05)\n",
      "(363, 4.5939833826671543e-05)\n",
      "(364, 4.3494683228413902e-05)\n",
      "(365, 4.1180239679074651e-05)\n",
      "(366, 3.8989563817753785e-05)\n",
      "(367, 3.6916612838889943e-05)\n",
      "(368, 3.4955247785667472e-05)\n",
      "(369, 3.3097587249331274e-05)\n",
      "(370, 3.1338542001323391e-05)\n",
      "(371, 2.9673451903450314e-05)\n",
      "(372, 2.8097087776044143e-05)\n",
      "(373, 2.6604614250457601e-05)\n",
      "(374, 2.5193395172599983e-05)\n",
      "(375, 2.3855881888764503e-05)\n",
      "(376, 2.2590007345532506e-05)\n",
      "(377, 2.1391038790898058e-05)\n",
      "(378, 2.0255963344768318e-05)\n",
      "(379, 1.9181403273378255e-05)\n",
      "(380, 1.8164865986509401e-05)\n",
      "(381, 1.7202144210080918e-05)\n",
      "(382, 1.6290695668621682e-05)\n",
      "(383, 1.5427306218083886e-05)\n",
      "(384, 1.4609753148567492e-05)\n",
      "(385, 1.3835770153649597e-05)\n",
      "(386, 1.3103069751121421e-05)\n",
      "(387, 1.2409648422129764e-05)\n",
      "(388, 1.1752442026999651e-05)\n",
      "(389, 1.1130241744134012e-05)\n",
      "(390, 1.054105015193544e-05)\n",
      "(391, 9.9830935769632854e-06)\n",
      "(392, 9.4548259269026651e-06)\n",
      "(393, 8.9552020779416822e-06)\n",
      "(394, 8.4816892333300759e-06)\n",
      "(395, 8.0331296883258006e-06)\n",
      "(396, 7.6083568996632627e-06)\n",
      "(397, 7.2061266088175616e-06)\n",
      "(398, 6.8253352040619451e-06)\n",
      "(399, 6.4648165243511228e-06)\n",
      "(400, 6.1235990803774508e-06)\n",
      "(401, 5.8001396092284996e-06)\n",
      "(402, 5.4937749881528718e-06)\n",
      "(403, 5.2036461472477811e-06)\n",
      "(404, 4.9288628049721038e-06)\n",
      "(405, 4.6686595564240612e-06)\n",
      "(406, 4.4225293058872462e-06)\n",
      "(407, 4.1892079150152742e-06)\n",
      "(408, 3.9681413806524533e-06)\n",
      "(409, 3.758781277691342e-06)\n",
      "(410, 3.5605213728735837e-06)\n",
      "(411, 3.372814531322075e-06)\n",
      "(412, 3.1950854840092093e-06)\n",
      "(413, 3.0267809509807825e-06)\n",
      "(414, 2.8672107367775638e-06)\n",
      "(415, 2.7160894945070775e-06)\n",
      "(416, 2.5729640151202001e-06)\n",
      "(417, 2.4374150310403185e-06)\n",
      "(418, 2.3090702138027965e-06)\n",
      "(419, 2.1876064799959874e-06)\n",
      "(420, 2.0724243972911605e-06)\n",
      "(421, 1.9632860627871498e-06)\n",
      "(422, 1.8599171149699906e-06)\n",
      "(423, 1.7620202627277127e-06)\n",
      "(424, 1.6693043519233128e-06)\n",
      "(425, 1.5815152561200585e-06)\n",
      "(426, 1.4983489130196133e-06)\n",
      "(427, 1.4195187436613579e-06)\n",
      "(428, 1.3448516596929146e-06)\n",
      "(429, 1.2741168366256815e-06)\n",
      "(430, 1.207118155441399e-06)\n",
      "(431, 1.1436513838159317e-06)\n",
      "(432, 1.0836082143200739e-06)\n",
      "(433, 1.0266634582482383e-06)\n",
      "(434, 9.7270921522574094e-07)\n",
      "(435, 9.216163046425998e-07)\n",
      "(436, 8.7319753650214031e-07)\n",
      "(437, 8.2733860498379358e-07)\n",
      "(438, 7.8391085567075153e-07)\n",
      "(439, 7.4276462440916022e-07)\n",
      "(440, 7.037604679984241e-07)\n",
      "(441, 6.6680611051476675e-07)\n",
      "(442, 6.3179798862653898e-07)\n",
      "(443, 5.9863681028358874e-07)\n",
      "(444, 5.6722169028142252e-07)\n",
      "(445, 5.3748842776134295e-07)\n",
      "(446, 5.0929676693279541e-07)\n",
      "(447, 4.8257513847784291e-07)\n",
      "(448, 4.5726052192120073e-07)\n",
      "(449, 4.3327845415297286e-07)\n",
      "(450, 4.1056285823799988e-07)\n",
      "(451, 3.8905715559911918e-07)\n",
      "(452, 3.6868286670498929e-07)\n",
      "(453, 3.4935542364918333e-07)\n",
      "(454, 3.3104158413012052e-07)\n",
      "(455, 3.1369017720683073e-07)\n",
      "(456, 2.9725310431741816e-07)\n",
      "(457, 2.8168098632298642e-07)\n",
      "(458, 2.6693443615435441e-07)\n",
      "(459, 2.5296008852449726e-07)\n",
      "(460, 2.3971042985713412e-07)\n",
      "(461, 2.2715629640579092e-07)\n",
      "(462, 2.1526113437510208e-07)\n",
      "(463, 2.0399456807255379e-07)\n",
      "(464, 1.9331701453744374e-07)\n",
      "(465, 1.832074251152781e-07)\n",
      "(466, 1.7361769289715187e-07)\n",
      "(467, 1.6453532325638299e-07)\n",
      "(468, 1.5592645883358938e-07)\n",
      "(469, 1.4776825348999066e-07)\n",
      "(470, 1.400397749749218e-07)\n",
      "(471, 1.3271673926008455e-07)\n",
      "(472, 1.2578120873338002e-07)\n",
      "(473, 1.1920305445062233e-07)\n",
      "(474, 1.1296925523514095e-07)\n",
      "(475, 1.0706268682084087e-07)\n",
      "(476, 1.0146793642059676e-07)\n",
      "(477, 9.6164201210927431e-08)\n",
      "(478, 9.1140344823051973e-08)\n",
      "(479, 8.6378528566782505e-08)\n",
      "(480, 8.1864416607454745e-08)\n",
      "(481, 7.7586372022610555e-08)\n",
      "(482, 7.3534012952188427e-08)\n",
      "(483, 6.9694988222477504e-08)\n",
      "(484, 6.605440647431347e-08)\n",
      "(485, 6.2607561028410222e-08)\n",
      "(486, 5.9338450678175966e-08)\n",
      "(487, 5.6239627334434888e-08)\n",
      "(488, 5.3303557681541688e-08)\n",
      "(489, 5.0521702430409163e-08)\n",
      "(490, 4.7884394976037224e-08)\n",
      "(491, 4.5385318871148612e-08)\n",
      "(492, 4.3018952701204295e-08)\n",
      "(493, 4.077359225597257e-08)\n",
      "(494, 3.8645695591551564e-08)\n",
      "(495, 3.6629483027374982e-08)\n",
      "(496, 3.4718754668785361e-08)\n",
      "(497, 3.2907714362841704e-08)\n",
      "(498, 3.119241123035609e-08)\n",
      "(499, 2.9567018330988184e-08)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N - размер батча; D_in - размерность входа;\n",
    "# H - скрытая размероность; D_out размерность выхода.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Инициализируем вход и выход из нормального распределения\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Инициализируем веса из нормального распределения\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "t1 = time()\n",
    "for t in range(500):\n",
    "\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    \n",
    "    # Используем функцию активации ReLU\n",
    "    \n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Считаем функцию потерь\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Считаем градиенты\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    \n",
    "    # Считаем композицию с производной ReLU\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Обновляем веса\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64606904984\n"
     ]
    }
   ],
   "source": [
    "print t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Первая и основная составляющая типичного современного фреймворка для машинного обучения - Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В интерфейсе базовых операций тензор ничем не отличается от np.array, но при этом тензоры можно эффективно использовать при обучении на gpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем неинициализированный тензор\n",
    "x = torch.Tensor(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 8.3316e-38  0.0000e+00 -5.5271e-38\n",
       " 4.5609e-41  2.8817e+24  4.5609e-41\n",
       " 2.9572e-38  0.0000e+00  2.9572e-38\n",
       " 0.0000e+00  0.0000e+00  0.0000e+00\n",
       " 0.0000e+00  0.0000e+00  0.0000e+00\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# инициализируем тензор нормальным распределением\n",
    "x = torch.randn(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.2670  0.0724  0.3699\n",
       "-0.3464 -1.5371 -0.5977\n",
       " 0.6510  0.0325  0.2199\n",
       " 0.5227 -0.6069  1.1274\n",
       "-0.3321 -1.5260  1.7369\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.9982  0.1485  0.4511\n",
       " 0.2239  0.6301  0.4746\n",
       " 0.4650  0.5401  0.4925\n",
       " 0.2356  0.9363  0.8141\n",
       " 0.9526  0.8683  0.1060\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7312  0.2209  0.8210\n",
       "-0.1224 -0.9071 -0.1231\n",
       " 1.1160  0.5726  0.7123\n",
       " 0.7582  0.3293  1.9415\n",
       " 0.6205 -0.6577  1.8429\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Первый способ сложить 2 тензора\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7312  0.2209  0.8210\n",
       "-0.1224 -0.9071 -0.1231\n",
       " 1.1160  0.5726  0.7123\n",
       " 0.7582  0.3293  1.9415\n",
       " 0.6205 -0.6577  1.8429\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Второй способ сложить 2 тензора\n",
    "x.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7312  0.2209  0.8210\n",
       "-0.1224 -0.9071 -0.1231\n",
       " 1.1160  0.5726  0.7123\n",
       " 0.7582  0.3293  1.9415\n",
       " 0.6205 -0.6577  1.8429\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# А еще можно так:\n",
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7312  0.2209  0.8210\n",
       "-0.1224 -0.9071 -0.1231\n",
       " 1.1160  0.5726  0.7123\n",
       " 0.7582  0.3293  1.9415\n",
       " 0.6205 -0.6577  1.8429\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраняем выход в тензор result\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Перевод из numpy в torch\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.randn(5, 3) \n",
    "b = torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0097  0.2683  0.6655  3.5459\n",
       "-1.3037  1.1624 -2.3810 -5.4057\n",
       "-3.2868  1.5457 -1.8646 -5.2660\n",
       "-2.8430  1.6398 -1.0075 -1.0834\n",
       " 2.1454 -1.7578  0.5441 -2.2567\n",
       "[torch.FloatTensor of size 5x4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Матричное умножение\n",
    "\n",
    "torch.mm(a,b)\n",
    "a.mm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# для python 3\n",
    "\n",
    "#a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# предостережение!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pytorch пока нет встроенной реализации broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = torch.randn(100, 10)\n",
    "x = torch.randn(1, 100)\n",
    "b = torch.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       "  6.9189  16.2675   5.6802   7.3530  -9.7121  -5.4097  10.4115  12.2918\n",
       "\n",
       "Columns 8 to 9 \n",
       "  8.9905  14.2689\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487343590888/work/torch/lib/TH/generic/THTensorMath.c:601",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-343405a772f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lysuhin/Distr/anaconda2/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m# TODO: add tests for operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0m__radd__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487343590888/work/torch/lib/TH/generic/THTensorMath.c:601"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 100)\n",
    "x.mm(W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " 25.1511  18.2276  -7.3328  13.9893  12.1010   6.6206  -6.2555   4.5219\n",
       " -3.6613  13.8094  -4.1052  -8.7536   6.3013  -7.3092   4.8149  -9.2200\n",
       "  2.2263  12.1097  -8.7588  10.8792  -8.4138  13.3245  10.8631   3.5090\n",
       " -1.7323   8.0811  19.0527 -13.2555 -19.1833   5.6016  22.9160   1.2069\n",
       "  6.9110  -5.8484   1.0999  -7.9457 -12.6434  13.1023   3.5895  -4.5340\n",
       "\n",
       "Columns 8 to 9 \n",
       "  7.4969  -0.6554\n",
       " -2.2016   9.1470\n",
       " -4.7548   2.9820\n",
       "  8.1227 -12.8620\n",
       "  1.7377   6.7146\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# walk-through\n",
    "\n",
    "x.mm(W) + b.repeat(x.size(0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяем пару строчек в обучении на np и код уже можно запускать и на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 35013889.247129336)\n",
      "(1, 32601090.006104976)\n",
      "(2, 31925385.040134504)\n",
      "(3, 28109765.607548773)\n",
      "(4, 20709786.573592227)\n",
      "(5, 12775438.678287297)\n",
      "(6, 7036610.602534682)\n",
      "(7, 3832782.9243932953)\n",
      "(8, 2243875.6232604478)\n",
      "(9, 1466885.1898228435)\n",
      "(10, 1061280.0613490958)\n",
      "(11, 825144.3961604855)\n",
      "(12, 670673.4630668685)\n",
      "(13, 559482.1940121669)\n",
      "(14, 474083.8116201181)\n",
      "(15, 405958.92519417964)\n",
      "(16, 350203.270184157)\n",
      "(17, 303891.2070024442)\n",
      "(18, 265036.91149549827)\n",
      "(19, 232226.7443207888)\n",
      "(20, 204334.70912032752)\n",
      "(21, 180489.78446389647)\n",
      "(22, 159986.99036683375)\n",
      "(23, 142266.05045526847)\n",
      "(24, 126887.30538930678)\n",
      "(25, 113479.2810834439)\n",
      "(26, 101748.07048603022)\n",
      "(27, 91455.90389786338)\n",
      "(28, 82389.19987829853)\n",
      "(29, 74391.40360661584)\n",
      "(30, 67304.68757790682)\n",
      "(31, 61008.35605036013)\n",
      "(32, 55398.09865702223)\n",
      "(33, 50387.44459377951)\n",
      "(34, 45899.51647750952)\n",
      "(35, 41874.610549579345)\n",
      "(36, 38261.57634348629)\n",
      "(37, 35012.28608932835)\n",
      "(38, 32081.654923820606)\n",
      "(39, 29430.546795348433)\n",
      "(40, 27030.93484589894)\n",
      "(41, 24855.563463432074)\n",
      "(42, 22879.17474410453)\n",
      "(43, 21082.937181529545)\n",
      "(44, 19447.612923761073)\n",
      "(45, 17956.100602917988)\n",
      "(46, 16594.69325032091)\n",
      "(47, 15348.731523795563)\n",
      "(48, 14207.645739765212)\n",
      "(49, 13161.282211689206)\n",
      "(50, 12201.291429166442)\n",
      "(51, 11319.078070981317)\n",
      "(52, 10507.75596596302)\n",
      "(53, 9761.389501861508)\n",
      "(54, 9072.923310200109)\n",
      "(55, 8438.07966195156)\n",
      "(56, 7852.503338483863)\n",
      "(57, 7311.526507807213)\n",
      "(58, 6811.152487058105)\n",
      "(59, 6348.673117865925)\n",
      "(60, 5920.400828657832)\n",
      "(61, 5523.829577399731)\n",
      "(62, 5156.578654412322)\n",
      "(63, 4815.75570016438)\n",
      "(64, 4499.1876013687)\n",
      "(65, 4205.060469686781)\n",
      "(66, 3931.6827124127685)\n",
      "(67, 3677.582924387189)\n",
      "(68, 3441.09218663504)\n",
      "(69, 3220.7633248876436)\n",
      "(70, 3015.4383766191613)\n",
      "(71, 2824.1868018605037)\n",
      "(72, 2645.9025140947633)\n",
      "(73, 2479.728048370611)\n",
      "(74, 2324.6732422780187)\n",
      "(75, 2179.849626850333)\n",
      "(76, 2044.5781505122577)\n",
      "(77, 1918.2467960930617)\n",
      "(78, 1800.118822162778)\n",
      "(79, 1689.6014018296366)\n",
      "(80, 1586.2725072476173)\n",
      "(81, 1489.6917243018765)\n",
      "(82, 1399.303673517235)\n",
      "(83, 1314.6664532873272)\n",
      "(84, 1235.3763703945233)\n",
      "(85, 1161.077551996601)\n",
      "(86, 1091.4585346920485)\n",
      "(87, 1026.1723828632214)\n",
      "(88, 964.9680193589952)\n",
      "(89, 907.6020290810866)\n",
      "(90, 853.7791556517099)\n",
      "(91, 803.2337150394906)\n",
      "(92, 755.8061075979049)\n",
      "(93, 711.2918881087826)\n",
      "(94, 669.4734335509875)\n",
      "(95, 630.2145704794357)\n",
      "(96, 593.3314934659975)\n",
      "(97, 558.6724369487893)\n",
      "(98, 526.1058026508927)\n",
      "(99, 495.50390979893234)\n",
      "(100, 466.74095928597944)\n",
      "(101, 439.6948822122524)\n",
      "(102, 414.26021496068194)\n",
      "(103, 390.3347523608286)\n",
      "(104, 367.8360983970809)\n",
      "(105, 346.6715002613556)\n",
      "(106, 326.7537100733073)\n",
      "(107, 308.00768464462135)\n",
      "(108, 290.3796656671566)\n",
      "(109, 273.78386463237854)\n",
      "(110, 258.15632769463)\n",
      "(111, 243.4447065937012)\n",
      "(112, 229.59272125245394)\n",
      "(113, 216.54515631795175)\n",
      "(114, 204.25183046583518)\n",
      "(115, 192.6755569473779)\n",
      "(116, 181.77115009083047)\n",
      "(117, 171.5007181339878)\n",
      "(118, 161.818881107538)\n",
      "(119, 152.70058558731762)\n",
      "(120, 144.10021140080963)\n",
      "(121, 135.9973274575132)\n",
      "(122, 128.36434380071537)\n",
      "(123, 121.16639596495037)\n",
      "(124, 114.383020926082)\n",
      "(125, 107.98454627123581)\n",
      "(126, 101.95365378339017)\n",
      "(127, 96.26297375037211)\n",
      "(128, 90.89574823132779)\n",
      "(129, 85.83369203047242)\n",
      "(130, 81.0581407643423)\n",
      "(131, 76.5523416268086)\n",
      "(132, 72.30285855135418)\n",
      "(133, 68.29265473447632)\n",
      "(134, 64.51565958945582)\n",
      "(135, 60.95613536424062)\n",
      "(136, 57.59353863360285)\n",
      "(137, 54.42035153251031)\n",
      "(138, 51.423049186115975)\n",
      "(139, 48.59658048285752)\n",
      "(140, 45.9265246731097)\n",
      "(141, 43.40537753305231)\n",
      "(142, 41.02402760573431)\n",
      "(143, 38.77652659960704)\n",
      "(144, 36.653430445561234)\n",
      "(145, 34.64919683015375)\n",
      "(146, 32.75498598469387)\n",
      "(147, 30.965995727448444)\n",
      "(148, 29.27671330491292)\n",
      "(149, 27.680570627538568)\n",
      "(150, 26.172786735425262)\n",
      "(151, 24.748177832688107)\n",
      "(152, 23.40200982411202)\n",
      "(153, 22.130291330680564)\n",
      "(154, 20.92825194325694)\n",
      "(155, 19.792402495108405)\n",
      "(156, 18.719659935600305)\n",
      "(157, 17.705460031107037)\n",
      "(158, 16.74693833909822)\n",
      "(159, 15.840978054731067)\n",
      "(160, 14.984323550312979)\n",
      "(161, 14.174318946851798)\n",
      "(162, 13.410063470671616)\n",
      "(163, 12.686834755566721)\n",
      "(164, 12.002981808636079)\n",
      "(165, 11.35645817841558)\n",
      "(166, 10.744727896245237)\n",
      "(167, 10.166818677518464)\n",
      "(168, 9.620427402989733)\n",
      "(169, 9.10345043086285)\n",
      "(170, 8.615080851669063)\n",
      "(171, 8.152562394860766)\n",
      "(172, 7.715433175065684)\n",
      "(173, 7.302287803204994)\n",
      "(174, 6.911379356083856)\n",
      "(175, 6.541190775080466)\n",
      "(176, 6.191398087875786)\n",
      "(177, 5.860486557808617)\n",
      "(178, 5.547365416147965)\n",
      "(179, 5.251459669960671)\n",
      "(180, 4.971535916903793)\n",
      "(181, 4.70642798573941)\n",
      "(182, 4.455480997246569)\n",
      "(183, 4.218390886801739)\n",
      "(184, 3.993928144745457)\n",
      "(185, 3.7814234957403166)\n",
      "(186, 3.580926835847851)\n",
      "(187, 3.3906256874104095)\n",
      "(188, 3.210516276413376)\n",
      "(189, 3.040298097058926)\n",
      "(190, 2.8790109942245437)\n",
      "(191, 2.7266090674638974)\n",
      "(192, 2.582008266012566)\n",
      "(193, 2.445367400522504)\n",
      "(194, 2.3160227904799635)\n",
      "(195, 2.193461493648769)\n",
      "(196, 2.0775780928546297)\n",
      "(197, 1.9678446313262576)\n",
      "(198, 1.8640708286388874)\n",
      "(199, 1.7657758158436074)\n",
      "(200, 1.6727431358353786)\n",
      "(201, 1.584633835848436)\n",
      "(202, 1.5012190264858702)\n",
      "(203, 1.4221752517672188)\n",
      "(204, 1.3472496116939041)\n",
      "(205, 1.2764201148424377)\n",
      "(206, 1.209357338194927)\n",
      "(207, 1.1458577156461267)\n",
      "(208, 1.085716001793653)\n",
      "(209, 1.0288896143448603)\n",
      "(210, 0.9749137197241365)\n",
      "(211, 0.9237892467169058)\n",
      "(212, 0.8753236828258775)\n",
      "(213, 0.8295621245138927)\n",
      "(214, 0.7861464041722508)\n",
      "(215, 0.744992539959096)\n",
      "(216, 0.706185805460013)\n",
      "(217, 0.6692917991592298)\n",
      "(218, 0.6343174780647312)\n",
      "(219, 0.6011959123617361)\n",
      "(220, 0.569916862842037)\n",
      "(221, 0.5401647026855461)\n",
      "(222, 0.5119713748139976)\n",
      "(223, 0.4852940674954189)\n",
      "(224, 0.4600440296358155)\n",
      "(225, 0.4361067991428538)\n",
      "(226, 0.4135154266574841)\n",
      "(227, 0.39195514313133106)\n",
      "(228, 0.37165075238636636)\n",
      "(229, 0.3523877394698234)\n",
      "(230, 0.3340630213561191)\n",
      "(231, 0.31674418367341417)\n",
      "(232, 0.300273358411566)\n",
      "(233, 0.284759181115793)\n",
      "(234, 0.26999959452317057)\n",
      "(235, 0.2559971856542662)\n",
      "(236, 0.24270805734420997)\n",
      "(237, 0.23015311985038545)\n",
      "(238, 0.2183065363869796)\n",
      "(239, 0.20702120808795854)\n",
      "(240, 0.19633923092078853)\n",
      "(241, 0.18617594094719814)\n",
      "(242, 0.1765757938237622)\n",
      "(243, 0.16752620914207506)\n",
      "(244, 0.15886966319445506)\n",
      "(245, 0.1506731836914812)\n",
      "(246, 0.1429317222763553)\n",
      "(247, 0.13555762760938372)\n",
      "(248, 0.1285687900210848)\n",
      "(249, 0.12193009975169744)\n",
      "(250, 0.11568402396816069)\n",
      "(251, 0.10971611334317188)\n",
      "(252, 0.10411871516303961)\n",
      "(253, 0.09876245674440298)\n",
      "(254, 0.0936821996221795)\n",
      "(255, 0.08888151807781597)\n",
      "(256, 0.08430829620510139)\n",
      "(257, 0.08000887710350568)\n",
      "(258, 0.07590047492173202)\n",
      "(259, 0.07201558412693121)\n",
      "(260, 0.06833711016023702)\n",
      "(261, 0.06483921383882763)\n",
      "(262, 0.061524672473898256)\n",
      "(263, 0.05838688613779386)\n",
      "(264, 0.05539908748394229)\n",
      "(265, 0.05259059057229898)\n",
      "(266, 0.04991877038928738)\n",
      "(267, 0.04735705353829367)\n",
      "(268, 0.044943309029972234)\n",
      "(269, 0.04266440279557315)\n",
      "(270, 0.0405050154781641)\n",
      "(271, 0.03845736252722176)\n",
      "(272, 0.03649875856567508)\n",
      "(273, 0.034657310079240666)\n",
      "(274, 0.032880436146066394)\n",
      "(275, 0.031199881003327157)\n",
      "(276, 0.029616727192272485)\n",
      "(277, 0.028095584344415947)\n",
      "(278, 0.026695246777732606)\n",
      "(279, 0.02534633835172162)\n",
      "(280, 0.024059819770159585)\n",
      "(281, 0.022843575679226902)\n",
      "(282, 0.021696909695055666)\n",
      "(283, 0.02059488784110819)\n",
      "(284, 0.019571087300857215)\n",
      "(285, 0.018583223873703147)\n",
      "(286, 0.017659281220268053)\n",
      "(287, 0.016765667376147597)\n",
      "(288, 0.015932495374129108)\n",
      "(289, 0.015131663299884579)\n",
      "(290, 0.014367351419335428)\n",
      "(291, 0.013661156587595702)\n",
      "(292, 0.012978594325624204)\n",
      "(293, 0.012332674860262571)\n",
      "(294, 0.01172261961429455)\n",
      "(295, 0.011149230129336551)\n",
      "(296, 0.010608431329479195)\n",
      "(297, 0.010072632590969466)\n",
      "(298, 0.009573237956411962)\n",
      "(299, 0.009103291670415459)\n",
      "(300, 0.0086533753523097)\n",
      "(301, 0.008229261709490343)\n",
      "(302, 0.00783753171265067)\n",
      "(303, 0.007453440563315761)\n",
      "(304, 0.007090890056499433)\n",
      "(305, 0.006747249592413462)\n",
      "(306, 0.006425039220996354)\n",
      "(307, 0.006109627455605615)\n",
      "(308, 0.005824157175833289)\n",
      "(309, 0.005542570906661437)\n",
      "(310, 0.005276160273199881)\n",
      "(311, 0.005025292950934013)\n",
      "(312, 0.0047871970784752055)\n",
      "(313, 0.004564575538765769)\n",
      "(314, 0.004344517663180447)\n",
      "(315, 0.004142119211643092)\n",
      "(316, 0.003945428370301096)\n",
      "(317, 0.003764285313442528)\n",
      "(318, 0.003586125325648004)\n",
      "(319, 0.0034245271143826805)\n",
      "(320, 0.003262370935431136)\n",
      "(321, 0.003116184131839894)\n",
      "(322, 0.0029729902027714195)\n",
      "(323, 0.002839033146255679)\n",
      "(324, 0.002709274637610115)\n",
      "(325, 0.0025856009443735034)\n",
      "(326, 0.0024726632057901626)\n",
      "(327, 0.0023643463532532216)\n",
      "(328, 0.0022605134127662296)\n",
      "(329, 0.002160192695805807)\n",
      "(330, 0.0020670787862247103)\n",
      "(331, 0.001976915171090665)\n",
      "(332, 0.001893528857920726)\n",
      "(333, 0.001813128058182581)\n",
      "(334, 0.0017330904804875313)\n",
      "(335, 0.0016609128351683378)\n",
      "(336, 0.0015929620036554404)\n",
      "(337, 0.0015272527210912351)\n",
      "(338, 0.0014606510298518227)\n",
      "(339, 0.0013993988867677665)\n",
      "(340, 0.0013456415801458288)\n",
      "(341, 0.0012903859293411785)\n",
      "(342, 0.0012382316692378798)\n",
      "(343, 0.0011872892763911264)\n",
      "(344, 0.00114188264598436)\n",
      "(345, 0.00109754838497228)\n",
      "(346, 0.0010521306836488997)\n",
      "(347, 0.0010108052565215853)\n",
      "(348, 0.0009736125973248133)\n",
      "(349, 0.0009360812846385819)\n",
      "(350, 0.0009007697411954063)\n",
      "(351, 0.0008649391012798824)\n",
      "(352, 0.0008324467209753217)\n",
      "(353, 0.0008013840552896778)\n",
      "(354, 0.0007728792683647373)\n",
      "(355, 0.0007438902237695971)\n",
      "(356, 0.0007177173665894876)\n",
      "(357, 0.0006922190383096272)\n",
      "(358, 0.0006661998069599218)\n",
      "(359, 0.0006438393483501059)\n",
      "(360, 0.0006217071947378083)\n",
      "(361, 0.0006005951587542135)\n",
      "(362, 0.0005804894181821729)\n",
      "(363, 0.0005610977497396141)\n",
      "(364, 0.0005408273326307639)\n",
      "(365, 0.0005219733912450869)\n",
      "(366, 0.0005057788783998651)\n",
      "(367, 0.0004883615917102707)\n",
      "(368, 0.00047187598590897606)\n",
      "(369, 0.00045712336117248453)\n",
      "(370, 0.0004421045886561781)\n",
      "(371, 0.00042803117443196426)\n",
      "(372, 0.000415150025863939)\n",
      "(373, 0.0004019716732891895)\n",
      "(374, 0.00038910266384256653)\n",
      "(375, 0.000377148083011869)\n",
      "(376, 0.00036655580559946666)\n",
      "(377, 0.0003541579261322897)\n",
      "(378, 0.00034309593375631)\n",
      "(379, 0.00033258528520874187)\n",
      "(380, 0.00032268403138663304)\n",
      "(381, 0.00031359574867544093)\n",
      "(382, 0.00030468195727258174)\n",
      "(383, 0.0002953325098830617)\n",
      "(384, 0.0002870070845145123)\n",
      "(385, 0.00027926174388305525)\n",
      "(386, 0.0002707554662329109)\n",
      "(387, 0.00026407644672922503)\n",
      "(388, 0.00025718155528937914)\n",
      "(389, 0.0002497320501099187)\n",
      "(390, 0.00024307735456470525)\n",
      "(391, 0.00023630411469766077)\n",
      "(392, 0.00023027962000343738)\n",
      "(393, 0.000224623819115366)\n",
      "(394, 0.00021884603329049013)\n",
      "(395, 0.00021333617484810274)\n",
      "(396, 0.00020722393524369442)\n",
      "(397, 0.00020243952883708394)\n",
      "(398, 0.00019756537456100398)\n",
      "(399, 0.0001928032952546732)\n",
      "(400, 0.00018763157194238989)\n",
      "(401, 0.0001831054376512098)\n",
      "(402, 0.00017878155145149843)\n",
      "(403, 0.00017452331756580286)\n",
      "(404, 0.00017019575653502727)\n",
      "(405, 0.0001663513175746978)\n",
      "(406, 0.00016222808778153242)\n",
      "(407, 0.00015906605928937855)\n",
      "(408, 0.00015617427314196242)\n",
      "(409, 0.00015179490970473907)\n",
      "(410, 0.00014834256383450262)\n",
      "(411, 0.0001447235414886866)\n",
      "(412, 0.000141125314548185)\n",
      "(413, 0.00013788298726342962)\n",
      "(414, 0.00013528901558829132)\n",
      "(415, 0.00013255878973082724)\n",
      "(416, 0.00012923053864309642)\n",
      "(417, 0.00012650032858807503)\n",
      "(418, 0.00012414575066604927)\n",
      "(419, 0.0001219159805328357)\n",
      "(420, 0.00011904231671915088)\n",
      "(421, 0.0001163293698201312)\n",
      "(422, 0.0001141277231359733)\n",
      "(423, 0.00011219416229450718)\n",
      "(424, 0.00010969591519068012)\n",
      "(425, 0.0001077325962079434)\n",
      "(426, 0.00010551266516371782)\n",
      "(427, 0.00010323794315718338)\n",
      "(428, 0.00010145397060741523)\n",
      "(429, 9.944261721346603e-05)\n",
      "(430, 9.769394118004648e-05)\n",
      "(431, 9.542296340488143e-05)\n",
      "(432, 9.384563951275737e-05)\n",
      "(433, 9.166499298113306e-05)\n",
      "(434, 8.991664143442879e-05)\n",
      "(435, 8.87970405744204e-05)\n",
      "(436, 8.675791388336462e-05)\n",
      "(437, 8.534464742748771e-05)\n",
      "(438, 8.381298310917995e-05)\n",
      "(439, 8.25256893861176e-05)\n",
      "(440, 8.09374089702844e-05)\n",
      "(441, 7.966548867767431e-05)\n",
      "(442, 7.821394346848275e-05)\n",
      "(443, 7.685975954284385e-05)\n",
      "(444, 7.541407576291265e-05)\n",
      "(445, 7.42956315475593e-05)\n",
      "(446, 7.339657421908535e-05)\n",
      "(447, 7.183065488212959e-05)\n",
      "(448, 7.066619201763491e-05)\n",
      "(449, 6.933250905989674e-05)\n",
      "(450, 6.79497820911118e-05)\n",
      "(451, 6.705704880226315e-05)\n",
      "(452, 6.607313701288697e-05)\n",
      "(453, 6.495990800224616e-05)\n",
      "(454, 6.378483596035378e-05)\n",
      "(455, 6.258669025460684e-05)\n",
      "(456, 6.170224768142774e-05)\n",
      "(457, 6.066510680689807e-05)\n",
      "(458, 5.9919138540184824e-05)\n",
      "(459, 5.8720272202106294e-05)\n",
      "(460, 5.792541289528774e-05)\n",
      "(461, 5.7064905293016666e-05)\n",
      "(462, 5.6397592596887924e-05)\n",
      "(463, 5.573002278572159e-05)\n",
      "(464, 5.461857221500388e-05)\n",
      "(465, 5.3793425509526815e-05)\n",
      "(466, 5.29805869355221e-05)\n",
      "(467, 5.236228958814559e-05)\n",
      "(468, 5.1410062758106e-05)\n",
      "(469, 5.059177620653221e-05)\n",
      "(470, 4.9955529878020766e-05)\n",
      "(471, 4.932839007387724e-05)\n",
      "(472, 4.855692742117279e-05)\n",
      "(473, 4.800000118304615e-05)\n",
      "(474, 4.762701270348557e-05)\n",
      "(475, 4.704153013616874e-05)\n",
      "(476, 4.6464089494713257e-05)\n",
      "(477, 4.5747180690947076e-05)\n",
      "(478, 4.5079766834585366e-05)\n",
      "(479, 4.438835657515072e-05)\n",
      "(480, 4.3888929472032934e-05)\n",
      "(481, 4.321814639045296e-05)\n",
      "(482, 4.272213295401805e-05)\n",
      "(483, 4.188786277775636e-05)\n",
      "(484, 4.144162933837647e-05)\n",
      "(485, 4.077189510130752e-05)\n",
      "(486, 4.046444196646737e-05)\n",
      "(487, 3.9919939678834115e-05)\n",
      "(488, 3.9391094643667635e-05)\n",
      "(489, 3.897885504326737e-05)\n",
      "(490, 3.8538806060459986e-05)\n",
      "(491, 3.830445587632656e-05)\n",
      "(492, 3.7601298957677144e-05)\n",
      "(493, 3.686354324709839e-05)\n",
      "(494, 3.648404276580808e-05)\n",
      "(495, 3.6078433473571425e-05)\n",
      "(496, 3.5734868024175914e-05)\n",
      "(497, 3.516202969446134e-05)\n",
      "(498, 3.477789758998817e-05)\n",
      "(499, 3.430682677219108e-05)\n",
      "0.776088953018\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "#dtype = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "t1 = time()\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "t2 = time()\n",
    "print t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (8) : invalid device function at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487343590888/work/torch/lib/THC/generic/THCTensorMath.cu:35",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-76e2fa56c3a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mh_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_relu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (8) : invalid device function at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487343590888/work/torch/lib/THC/generic/THCTensorMath.cu:35"
     ]
    }
   ],
   "source": [
    "#dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in).type(dtype)\n",
    "y = torch.randn(N, D_out).type(dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H).type(dtype)\n",
    "w2 = torch.randn(H, D_out).type(dtype)\n",
    "\n",
    "t1 = time()\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "t2 = time()\n",
    "print t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918998003006\n"
     ]
    }
   ],
   "source": [
    "print t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Но самое важное в фреймворках - графы вычисления и автоматическое дифференцирование "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable - обертка над тензором, содержащая значения градиента и еще немного полезной информации\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# операция, которая породила переменную.\n",
    "x.creator is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd._functions.basic_ops.AddConstant at 0x7f245e043d98>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y.creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Function in module torch.autograd.function:\n",
      "\n",
      "class Function(torch._C._FunctionBase)\n",
      " |  Records operation history and defines formulas for differentiating ops.\n",
      " |  \n",
      " |  Every operation performed on :class:`Variable` s creates a new function\n",
      " |  object, that performs the computation, and records that it happened.\n",
      " |  The history is retained in the form of a DAG of functions, with edges\n",
      " |  denoting data dependencies (``input <- output``). Then, when backward is\n",
      " |  called, the graph is processed in the topological ordering, by calling\n",
      " |  :func:`backward` methods of each :class:`Function` object, and passing\n",
      " |  returned gradients on to next :class:`Function` s.\n",
      " |  \n",
      " |  Normally, the only way users interact with functions is by creating\n",
      " |  subclasses and defining new operations. This is a recommended way of\n",
      " |  extending torch.autograd.\n",
      " |  \n",
      " |  Since Function logic is a hotspot in most scripts, almost all of it\n",
      " |  was moved to our C backend, to ensure that the framework overhead is\n",
      " |  minimal.\n",
      " |  \n",
      " |  Each function is meant to be used only once (in the forward pass).\n",
      " |  \n",
      " |  Attributes:\n",
      " |      saved_tensors: Tuple of Tensors that were saved in the call to\n",
      " |          :func:`forward`.\n",
      " |      needs_input_grad: Tuple of booleans of length :attr:`num_inputs`,\n",
      " |          indicating whether a given input requires gradient. This can be\n",
      " |          used to optimize buffers saved for backward, and ignoring gradient\n",
      " |          computation in :func:`~Function.backward`.\n",
      " |      num_inputs: Number of inputs given to :func:`forward`.\n",
      " |      num_outputs: Number of tensors returned by :func:`forward`.\n",
      " |      requires_grad: Boolean indicating whether the :func:`backward` will\n",
      " |          ever need to be called.\n",
      " |      previous_functions: Tuple of (int, Function) pairs of length\n",
      " |          :attr:`num_inputs`. Each entry contains a reference to a\n",
      " |          :class:`Function` that created corresponding input, and an index\n",
      " |          of the previous function output that's been used.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Function\n",
      " |      torch._C._FunctionBase\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__ = _do_forward(...)\n",
      " |  \n",
      " |  backward(self, *grad_output)\n",
      " |      Defines a formula for differentiating the operation.\n",
      " |      \n",
      " |      This function is to be overriden by all subclasses.\n",
      " |      \n",
      " |      All arguments are tensors. It has to accept exactly as many arguments,\n",
      " |      as many outputs did :func:`forward` return, and it should return as\n",
      " |      many tensors, as there were inputs to :func:`forward`. Each argument\n",
      " |      is the gradient w.r.t the given output, and each returned value should\n",
      " |      be the gradient w.r.t. the corresponding input.\n",
      " |  \n",
      " |  forward(self, *input)\n",
      " |      Performs the operation.\n",
      " |      \n",
      " |      This function is to be overriden by all subclasses.\n",
      " |      \n",
      " |      It can take and return an arbitrary number of tensors.\n",
      " |  \n",
      " |  mark_dirty(self, *args)\n",
      " |      Marks given tensors as modified in an in-place operation.\n",
      " |      \n",
      " |      **This should be called at most once, only from inside the**\n",
      " |      :func:`forward` **method, and all arguments should be inputs.**\n",
      " |      \n",
      " |      Every tensor that's been modified in-place in a call to :func:`forward`\n",
      " |      should be given to this function, to ensure correcness of our checks.\n",
      " |      It doesn't matter wheter the function is called before or after\n",
      " |      modification.\n",
      " |  \n",
      " |  mark_non_differentiable(self, *args)\n",
      " |      Marks outputs as non-differentiable.\n",
      " |      \n",
      " |      **This should be called at most once, only from inside the**\n",
      " |      :func:`forward` **method, and all arguments should be outputs.**\n",
      " |      \n",
      " |      This will mark outputs as not requiring gradients, increasing the\n",
      " |      efficiency of backward computation. You still need to accept a gradient\n",
      " |      for each output in :meth:`~Function.backward`, but it's always going to\n",
      " |      be ``None``.\n",
      " |      \n",
      " |      This is used e.g. for indices returned from a max :class:`Function`.\n",
      " |  \n",
      " |  mark_shared_storage(self, *pairs)\n",
      " |      Marks that given pairs of distinct tensors are sharing storage.\n",
      " |      \n",
      " |      **This should be called at most once, only from inside the**\n",
      " |      :func:`forward` **method, and all arguments should be pairs of\n",
      " |      (input, output).**\n",
      " |      \n",
      " |      If some of the outputs are going to be tensors sharing storage with\n",
      " |      some of the inputs, all pairs of (input_arg, output_arg) should be\n",
      " |      given to this function, to ensure correctness checking of in-place\n",
      " |      modification. The only exception is when an output is exactly the same\n",
      " |      tensor as input (e.g. in-place ops). In such case it's easy to conclude\n",
      " |      that they're sharing data, so we don't require specifying such\n",
      " |      dependencies.\n",
      " |      \n",
      " |      This function is not needed in most functions. It's primarily used in\n",
      " |      indexing and transpose ops.\n",
      " |  \n",
      " |  register_hook(self, hook)\n",
      " |  \n",
      " |  save_for_backward(self, *tensors)\n",
      " |      Saves given tensors for a future call to :func:`~Function.backward`.\n",
      " |      \n",
      " |      **This should be called at most once, and only from inside the**\n",
      " |      :func:`forward` **method.**\n",
      " |      \n",
      " |      Later, saved tensors can be accessed through the :attr:`saved_tensors`\n",
      " |      attribute. Before returning them to the user, a check is made, to\n",
      " |      ensure they weren't used in any in-place operation that modified\n",
      " |      their content.\n",
      " |      \n",
      " |      Arguments can also be ``None``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C._FunctionBase:\n",
      " |  \n",
      " |  dirty_tensors\n",
      " |  \n",
      " |  needs_input_grad\n",
      " |  \n",
      " |  non_differentiable\n",
      " |  \n",
      " |  num_inputs\n",
      " |  \n",
      " |  num_outputs\n",
      " |  \n",
      " |  previous_functions\n",
      " |  \n",
      " |  requires_grad\n",
      " |  \n",
      " |  saved_tensors\n",
      " |  \n",
      " |  shared_pairs\n",
      " |  \n",
      " |  to_save\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch._C._FunctionBase:\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.autograd.Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 18  18\n",
       " 18  18\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 18\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.mean()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Запускаем бэкпроп\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что произошло?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd строит ациклический граф высчисления из переменных и операций(функций)\n",
    "out.backward проходит по всему графу начиная от вершины out и считает градиенты вершин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    def forward(self, input):\n",
    "        # forward pass\n",
    "        self.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # backward pass\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим сохранить значения переменных в графе, то используем retain_variables = True. \n",
    "Это может быть нужно, если мы хотим несколько раз подряд сделать backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "y.backward(torch.ones(2, 2))\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.1863  1.3897\n",
       " 1.9748  0.3825\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = torch.randn(2, 2)\n",
    "\n",
    "y.backward(gradient)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "y = x + 2\n",
    "y.backward(torch.ones(2, 2), retain_variables=True)\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2347  0.3023\n",
       "-0.7550  1.8709\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = torch.randn(2, 2)\n",
    "\n",
    "y.backward(gradient)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Снова вернемся к исходной двухслойной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 34765560.0)\n",
      "(1, 32018410.0)\n",
      "(2, 32139958.0)\n",
      "(3, 29572438.0)\n",
      "(4, 22981848.0)\n",
      "(5, 14632168.0)\n",
      "(6, 8165254.0)\n",
      "(7, 4402506.5)\n",
      "(8, 2550213.5)\n",
      "(9, 1660576.75)\n",
      "(10, 1207646.0)\n",
      "(11, 947982.375)\n",
      "(12, 778737.125)\n",
      "(13, 656311.0)\n",
      "(14, 561535.6875)\n",
      "(15, 485094.90625)\n",
      "(16, 421942.25)\n",
      "(17, 369043.09375)\n",
      "(18, 324302.90625)\n",
      "(19, 286191.125)\n",
      "(20, 253461.84375)\n",
      "(21, 225251.78125)\n",
      "(22, 200827.734375)\n",
      "(23, 179603.28125)\n",
      "(24, 161029.703125)\n",
      "(25, 144752.984375)\n",
      "(26, 130422.921875)\n",
      "(27, 117763.390625)\n",
      "(28, 106607.0)\n",
      "(29, 96689.2578125)\n",
      "(30, 87852.7421875)\n",
      "(31, 79961.9609375)\n",
      "(32, 72906.7265625)\n",
      "(33, 66570.96875)\n",
      "(34, 60881.53125)\n",
      "(35, 55750.91015625)\n",
      "(36, 51116.375)\n",
      "(37, 46924.109375)\n",
      "(38, 43124.81640625)\n",
      "(39, 39677.63671875)\n",
      "(40, 36542.8515625)\n",
      "(41, 33686.5234375)\n",
      "(42, 31083.400390625)\n",
      "(43, 28707.677734375)\n",
      "(44, 26536.044921875)\n",
      "(45, 24549.94921875)\n",
      "(46, 22731.83984375)\n",
      "(47, 21065.1875)\n",
      "(48, 19534.98046875)\n",
      "(49, 18129.05859375)\n",
      "(50, 16836.48046875)\n",
      "(51, 15646.4453125)\n",
      "(52, 14550.3330078125)\n",
      "(53, 13540.3662109375)\n",
      "(54, 12610.232421875)\n",
      "(55, 11751.0517578125)\n",
      "(56, 10956.80859375)\n",
      "(57, 10221.9921875)\n",
      "(58, 9541.888671875)\n",
      "(59, 8911.9248046875)\n",
      "(60, 8327.361328125)\n",
      "(61, 7784.83447265625)\n",
      "(62, 7281.03857421875)\n",
      "(63, 6812.97802734375)\n",
      "(64, 6377.7724609375)\n",
      "(65, 5973.09912109375)\n",
      "(66, 5596.60888671875)\n",
      "(67, 5245.8857421875)\n",
      "(68, 4919.041015625)\n",
      "(69, 4614.30517578125)\n",
      "(70, 4330.12841796875)\n",
      "(71, 4064.913818359375)\n",
      "(72, 3817.20166015625)\n",
      "(73, 3585.807861328125)\n",
      "(74, 3369.44921875)\n",
      "(75, 3167.230224609375)\n",
      "(76, 2978.16552734375)\n",
      "(77, 2801.22265625)\n",
      "(78, 2635.593017578125)\n",
      "(79, 2480.392822265625)\n",
      "(80, 2335.01611328125)\n",
      "(81, 2198.72265625)\n",
      "(82, 2070.942138671875)\n",
      "(83, 1951.07421875)\n",
      "(84, 1838.590576171875)\n",
      "(85, 1732.9873046875)\n",
      "(86, 1633.9246826171875)\n",
      "(87, 1540.8646240234375)\n",
      "(88, 1453.4083251953125)\n",
      "(89, 1371.2430419921875)\n",
      "(90, 1293.98046875)\n",
      "(91, 1221.3560791015625)\n",
      "(92, 1153.031494140625)\n",
      "(93, 1088.7274169921875)\n",
      "(94, 1028.222900390625)\n",
      "(95, 971.249267578125)\n",
      "(96, 917.5879516601562)\n",
      "(97, 867.1234741210938)\n",
      "(98, 819.573974609375)\n",
      "(99, 774.7879028320312)\n",
      "(100, 732.55517578125)\n",
      "(101, 692.7347412109375)\n",
      "(102, 655.1891479492188)\n",
      "(103, 619.9663696289062)\n",
      "(104, 586.7881469726562)\n",
      "(105, 555.47412109375)\n",
      "(106, 525.9195556640625)\n",
      "(107, 498.0113525390625)\n",
      "(108, 471.64715576171875)\n",
      "(109, 446.755859375)\n",
      "(110, 423.2375793457031)\n",
      "(111, 401.0231628417969)\n",
      "(112, 380.0146484375)\n",
      "(113, 360.16082763671875)\n",
      "(114, 341.3886413574219)\n",
      "(115, 323.6301574707031)\n",
      "(116, 306.8341064453125)\n",
      "(117, 290.94671630859375)\n",
      "(118, 275.9161682128906)\n",
      "(119, 261.69403076171875)\n",
      "(120, 248.23098754882812)\n",
      "(121, 235.4884490966797)\n",
      "(122, 223.42807006835938)\n",
      "(123, 212.00424194335938)\n",
      "(124, 201.18911743164062)\n",
      "(125, 190.94142150878906)\n",
      "(126, 181.23851013183594)\n",
      "(127, 172.04632568359375)\n",
      "(128, 163.33572387695312)\n",
      "(129, 155.0807342529297)\n",
      "(130, 147.2572479248047)\n",
      "(131, 139.84658813476562)\n",
      "(132, 132.838623046875)\n",
      "(133, 126.19642639160156)\n",
      "(134, 119.8974380493164)\n",
      "(135, 113.92341613769531)\n",
      "(136, 108.2549819946289)\n",
      "(137, 102.87847137451172)\n",
      "(138, 97.77761840820312)\n",
      "(139, 92.93665313720703)\n",
      "(140, 88.3430404663086)\n",
      "(141, 83.98399353027344)\n",
      "(142, 79.8468246459961)\n",
      "(143, 75.9186782836914)\n",
      "(144, 72.18993377685547)\n",
      "(145, 68.6486587524414)\n",
      "(146, 65.28523254394531)\n",
      "(147, 62.092655181884766)\n",
      "(148, 59.06034851074219)\n",
      "(149, 56.17965316772461)\n",
      "(150, 53.443138122558594)\n",
      "(151, 50.84349060058594)\n",
      "(152, 48.373714447021484)\n",
      "(153, 46.02764129638672)\n",
      "(154, 43.79811096191406)\n",
      "(155, 41.678367614746094)\n",
      "(156, 39.6641845703125)\n",
      "(157, 37.75014877319336)\n",
      "(158, 35.93144226074219)\n",
      "(159, 34.201725006103516)\n",
      "(160, 32.5569953918457)\n",
      "(161, 30.992843627929688)\n",
      "(162, 29.505565643310547)\n",
      "(163, 28.09170150756836)\n",
      "(164, 26.746856689453125)\n",
      "(165, 25.467432022094727)\n",
      "(166, 24.250755310058594)\n",
      "(167, 23.09369468688965)\n",
      "(168, 21.99260139465332)\n",
      "(169, 20.945276260375977)\n",
      "(170, 19.949342727661133)\n",
      "(171, 19.001140594482422)\n",
      "(172, 18.09946632385254)\n",
      "(173, 17.240936279296875)\n",
      "(174, 16.42445945739746)\n",
      "(175, 15.647500991821289)\n",
      "(176, 14.907000541687012)\n",
      "(177, 14.202767372131348)\n",
      "(178, 13.532570838928223)\n",
      "(179, 12.894448280334473)\n",
      "(180, 12.286792755126953)\n",
      "(181, 11.70865535736084)\n",
      "(182, 11.157707214355469)\n",
      "(183, 10.633467674255371)\n",
      "(184, 10.134506225585938)\n",
      "(185, 9.659479141235352)\n",
      "(186, 9.206233978271484)\n",
      "(187, 8.775080680847168)\n",
      "(188, 8.364706039428711)\n",
      "(189, 7.973723411560059)\n",
      "(190, 7.601304531097412)\n",
      "(191, 7.246126174926758)\n",
      "(192, 6.908076286315918)\n",
      "(193, 6.586116790771484)\n",
      "(194, 6.2795915603637695)\n",
      "(195, 5.9873223304748535)\n",
      "(196, 5.708974838256836)\n",
      "(197, 5.443637371063232)\n",
      "(198, 5.190855979919434)\n",
      "(199, 4.94999361038208)\n",
      "(200, 4.720850467681885)\n",
      "(201, 4.5020270347595215)\n",
      "(202, 4.293643474578857)\n",
      "(203, 4.094819068908691)\n",
      "(204, 3.9054486751556396)\n",
      "(205, 3.725092649459839)\n",
      "(206, 3.553072690963745)\n",
      "(207, 3.3891241550445557)\n",
      "(208, 3.232891798019409)\n",
      "(209, 3.0839366912841797)\n",
      "(210, 2.9418935775756836)\n",
      "(211, 2.8063466548919678)\n",
      "(212, 2.677349090576172)\n",
      "(213, 2.554370403289795)\n",
      "(214, 2.4370527267456055)\n",
      "(215, 2.325294256210327)\n",
      "(216, 2.218405246734619)\n",
      "(217, 2.1168174743652344)\n",
      "(218, 2.0197818279266357)\n",
      "(219, 1.9273014068603516)\n",
      "(220, 1.8390731811523438)\n",
      "(221, 1.7548964023590088)\n",
      "(222, 1.6746180057525635)\n",
      "(223, 1.5981541872024536)\n",
      "(224, 1.5251367092132568)\n",
      "(225, 1.4556432962417603)\n",
      "(226, 1.3891957998275757)\n",
      "(227, 1.3258824348449707)\n",
      "(228, 1.2653957605361938)\n",
      "(229, 1.2078630924224854)\n",
      "(230, 1.1528109312057495)\n",
      "(231, 1.1003214120864868)\n",
      "(232, 1.0503711700439453)\n",
      "(233, 1.0025866031646729)\n",
      "(234, 0.9570465683937073)\n",
      "(235, 0.9137054681777954)\n",
      "(236, 0.872235894203186)\n",
      "(237, 0.8326857089996338)\n",
      "(238, 0.794864296913147)\n",
      "(239, 0.7589097619056702)\n",
      "(240, 0.724557638168335)\n",
      "(241, 0.6917934417724609)\n",
      "(242, 0.6604647040367126)\n",
      "(243, 0.6306225657463074)\n",
      "(244, 0.6020631790161133)\n",
      "(245, 0.574834406375885)\n",
      "(246, 0.5488969087600708)\n",
      "(247, 0.5241633653640747)\n",
      "(248, 0.500586211681366)\n",
      "(249, 0.47802990674972534)\n",
      "(250, 0.4563694894313812)\n",
      "(251, 0.43579748272895813)\n",
      "(252, 0.416161447763443)\n",
      "(253, 0.397483229637146)\n",
      "(254, 0.3795710504055023)\n",
      "(255, 0.3625361919403076)\n",
      "(256, 0.3462563753128052)\n",
      "(257, 0.33073773980140686)\n",
      "(258, 0.31584393978118896)\n",
      "(259, 0.30169928073883057)\n",
      "(260, 0.28813278675079346)\n",
      "(261, 0.2751480042934418)\n",
      "(262, 0.2628445625305176)\n",
      "(263, 0.2510722875595093)\n",
      "(264, 0.23983214795589447)\n",
      "(265, 0.22905591130256653)\n",
      "(266, 0.2188086062669754)\n",
      "(267, 0.20897270739078522)\n",
      "(268, 0.1996520757675171)\n",
      "(269, 0.19073082506656647)\n",
      "(270, 0.18221302330493927)\n",
      "(271, 0.1741059571504593)\n",
      "(272, 0.16628879308700562)\n",
      "(273, 0.1588515341281891)\n",
      "(274, 0.15178126096725464)\n",
      "(275, 0.1450214684009552)\n",
      "(276, 0.13855518400669098)\n",
      "(277, 0.13235194981098175)\n",
      "(278, 0.12646521627902985)\n",
      "(279, 0.12083104252815247)\n",
      "(280, 0.11548298597335815)\n",
      "(281, 0.11030331254005432)\n",
      "(282, 0.10539732128381729)\n",
      "(283, 0.10071888566017151)\n",
      "(284, 0.09624875336885452)\n",
      "(285, 0.09197521954774857)\n",
      "(286, 0.08790402859449387)\n",
      "(287, 0.08400070667266846)\n",
      "(288, 0.08027587085962296)\n",
      "(289, 0.07669440656900406)\n",
      "(290, 0.07328295707702637)\n",
      "(291, 0.07003306597471237)\n",
      "(292, 0.06693682819604874)\n",
      "(293, 0.06398522108793259)\n",
      "(294, 0.06116476655006409)\n",
      "(295, 0.058444131165742874)\n",
      "(296, 0.0558706633746624)\n",
      "(297, 0.053406063467264175)\n",
      "(298, 0.05105244740843773)\n",
      "(299, 0.048802513629198074)\n",
      "(300, 0.046630483120679855)\n",
      "(301, 0.04458662495017052)\n",
      "(302, 0.04262828826904297)\n",
      "(303, 0.04075685515999794)\n",
      "(304, 0.038958534598350525)\n",
      "(305, 0.03724544867873192)\n",
      "(306, 0.035602252930402756)\n",
      "(307, 0.034024257212877274)\n",
      "(308, 0.03253891319036484)\n",
      "(309, 0.031105585396289825)\n",
      "(310, 0.02973972074687481)\n",
      "(311, 0.02844073250889778)\n",
      "(312, 0.027185944840312004)\n",
      "(313, 0.02599959261715412)\n",
      "(314, 0.02486281469464302)\n",
      "(315, 0.02376990020275116)\n",
      "(316, 0.022727234289050102)\n",
      "(317, 0.021734636276960373)\n",
      "(318, 0.02079557627439499)\n",
      "(319, 0.019889114424586296)\n",
      "(320, 0.01901426911354065)\n",
      "(321, 0.01819945126771927)\n",
      "(322, 0.0174112506210804)\n",
      "(323, 0.016653861850500107)\n",
      "(324, 0.015929708257317543)\n",
      "(325, 0.015247895382344723)\n",
      "(326, 0.014583691954612732)\n",
      "(327, 0.01395754236727953)\n",
      "(328, 0.013347274623811245)\n",
      "(329, 0.012780191376805305)\n",
      "(330, 0.012225732207298279)\n",
      "(331, 0.011696772649884224)\n",
      "(332, 0.0112009821459651)\n",
      "(333, 0.01073144469410181)\n",
      "(334, 0.010273337364196777)\n",
      "(335, 0.009831280447542667)\n",
      "(336, 0.009421708062291145)\n",
      "(337, 0.009018108248710632)\n",
      "(338, 0.008633905090391636)\n",
      "(339, 0.00827089324593544)\n",
      "(340, 0.007922188378870487)\n",
      "(341, 0.007586616091430187)\n",
      "(342, 0.0072690583765506744)\n",
      "(343, 0.006965167820453644)\n",
      "(344, 0.006674682255834341)\n",
      "(345, 0.0064064981415867805)\n",
      "(346, 0.006132704205811024)\n",
      "(347, 0.005881837103515863)\n",
      "(348, 0.005641740281134844)\n",
      "(349, 0.005410884041339159)\n",
      "(350, 0.005187304224818945)\n",
      "(351, 0.00497917365282774)\n",
      "(352, 0.004771907813847065)\n",
      "(353, 0.004577331244945526)\n",
      "(354, 0.004391199443489313)\n",
      "(355, 0.004215389024466276)\n",
      "(356, 0.004046425689011812)\n",
      "(357, 0.0038813853170722723)\n",
      "(358, 0.003730007214471698)\n",
      "(359, 0.0035855229943990707)\n",
      "(360, 0.0034401388838887215)\n",
      "(361, 0.003303944831714034)\n",
      "(362, 0.003172755939885974)\n",
      "(363, 0.0030541217420250177)\n",
      "(364, 0.0029338053427636623)\n",
      "(365, 0.002819945802912116)\n",
      "(366, 0.002714807167649269)\n",
      "(367, 0.002608239185065031)\n",
      "(368, 0.0025124752428382635)\n",
      "(369, 0.0024178759194910526)\n",
      "(370, 0.0023267341312021017)\n",
      "(371, 0.002239227294921875)\n",
      "(372, 0.002159337280318141)\n",
      "(373, 0.002076017437502742)\n",
      "(374, 0.002001553773880005)\n",
      "(375, 0.001925584627315402)\n",
      "(376, 0.001855130190961063)\n",
      "(377, 0.0017864112742245197)\n",
      "(378, 0.0017233459511771798)\n",
      "(379, 0.0016620176611468196)\n",
      "(380, 0.0016033132560551167)\n",
      "(381, 0.001546124811284244)\n",
      "(382, 0.0014910218305885792)\n",
      "(383, 0.001438736799173057)\n",
      "(384, 0.0013901543570682406)\n",
      "(385, 0.0013417147565633059)\n",
      "(386, 0.0012965280329808593)\n",
      "(387, 0.0012536103604361415)\n",
      "(388, 0.0012099589221179485)\n",
      "(389, 0.00116924534086138)\n",
      "(390, 0.00113028718624264)\n",
      "(391, 0.0010953411692753434)\n",
      "(392, 0.001059220521710813)\n",
      "(393, 0.0010234429500997066)\n",
      "(394, 0.0009906493360176682)\n",
      "(395, 0.0009580365149304271)\n",
      "(396, 0.0009284340194426477)\n",
      "(397, 0.0008988514891825616)\n",
      "(398, 0.0008691164548508823)\n",
      "(399, 0.0008423612453043461)\n",
      "(400, 0.0008165868348442018)\n",
      "(401, 0.0007912992732599378)\n",
      "(402, 0.0007689996855333447)\n",
      "(403, 0.0007441416964866221)\n",
      "(404, 0.0007231610943563282)\n",
      "(405, 0.0006995144067332149)\n",
      "(406, 0.0006772865890525281)\n",
      "(407, 0.0006575763691216707)\n",
      "(408, 0.0006380413542501628)\n",
      "(409, 0.0006198697374202311)\n",
      "(410, 0.0006016446859575808)\n",
      "(411, 0.0005847066640853882)\n",
      "(412, 0.0005674842977896333)\n",
      "(413, 0.0005519089172594249)\n",
      "(414, 0.0005361212533898652)\n",
      "(415, 0.0005199990700930357)\n",
      "(416, 0.0005056139198131859)\n",
      "(417, 0.0004919093917123973)\n",
      "(418, 0.0004767295904457569)\n",
      "(419, 0.0004644321743398905)\n",
      "(420, 0.0004515577165875584)\n",
      "(421, 0.00043912214459851384)\n",
      "(422, 0.00042666468652896583)\n",
      "(423, 0.00041592371417209506)\n",
      "(424, 0.0004039181803818792)\n",
      "(425, 0.0003951968683395535)\n",
      "(426, 0.0003843654994852841)\n",
      "(427, 0.00037357283872552216)\n",
      "(428, 0.0003636717447079718)\n",
      "(429, 0.0003551066911313683)\n",
      "(430, 0.00034536889870651066)\n",
      "(431, 0.0003372587088961154)\n",
      "(432, 0.00032798905158415437)\n",
      "(433, 0.0003195965546183288)\n",
      "(434, 0.00031208398286253214)\n",
      "(435, 0.0003046792990062386)\n",
      "(436, 0.00029734274721704423)\n",
      "(437, 0.0002903785207308829)\n",
      "(438, 0.00028310116613283753)\n",
      "(439, 0.0002767182304523885)\n",
      "(440, 0.00027048029005527496)\n",
      "(441, 0.0002643486368469894)\n",
      "(442, 0.00025840874877758324)\n",
      "(443, 0.00025266699958592653)\n",
      "(444, 0.00024728369317017496)\n",
      "(445, 0.00024148024385794997)\n",
      "(446, 0.00023565035371575505)\n",
      "(447, 0.00023009676078800112)\n",
      "(448, 0.0002258261520182714)\n",
      "(449, 0.00022059179900679737)\n",
      "(450, 0.00021590743563137949)\n",
      "(451, 0.0002114617236657068)\n",
      "(452, 0.0002065400331048295)\n",
      "(453, 0.0002020666579483077)\n",
      "(454, 0.0001976702915271744)\n",
      "(455, 0.00019372256065253168)\n",
      "(456, 0.00018961796013172716)\n",
      "(457, 0.00018551717221271247)\n",
      "(458, 0.0001818918826756999)\n",
      "(459, 0.00017809693235903978)\n",
      "(460, 0.00017439694784116)\n",
      "(461, 0.0001704203459667042)\n",
      "(462, 0.00016720277199056)\n",
      "(463, 0.00016390909149777144)\n",
      "(464, 0.00016014196444302797)\n",
      "(465, 0.00015722298121545464)\n",
      "(466, 0.00015426264144480228)\n",
      "(467, 0.00015088616055436432)\n",
      "(468, 0.00014754454605281353)\n",
      "(469, 0.00014524938887916505)\n",
      "(470, 0.0001420215703547001)\n",
      "(471, 0.00013907856191508472)\n",
      "(472, 0.00013691031199414283)\n",
      "(473, 0.00013410949031822383)\n",
      "(474, 0.00013171373575460166)\n",
      "(475, 0.00012911550584249198)\n",
      "(476, 0.000126795464893803)\n",
      "(477, 0.00012433096708264202)\n",
      "(478, 0.00012240830983500928)\n",
      "(479, 0.00012021891598124057)\n",
      "(480, 0.00011818219354609028)\n",
      "(481, 0.00011529805487953126)\n",
      "(482, 0.00011322840146021917)\n",
      "(483, 0.00011148031626362354)\n",
      "(484, 0.00010934624151559547)\n",
      "(485, 0.00010742651647888124)\n",
      "(486, 0.00010572958126431331)\n",
      "(487, 0.00010369556548539549)\n",
      "(488, 0.0001019491974147968)\n",
      "(489, 0.00010009803372668102)\n",
      "(490, 9.861144644673914e-05)\n",
      "(491, 9.69112734310329e-05)\n",
      "(492, 9.524889901513234e-05)\n",
      "(493, 9.358060924569145e-05)\n",
      "(494, 9.242832311429083e-05)\n",
      "(495, 9.114242129726335e-05)\n",
      "(496, 8.973538933787495e-05)\n",
      "(497, 8.851030725054443e-05)\n",
      "(498, 8.702089689904824e-05)\n",
      "(499, 8.526201418135315e-05)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "#dtype = torch.cuda.FloatTensor # GPU\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "t1 = time()\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    # Обнуляем градиенты\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "t2 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96022105217\n"
     ]
    }
   ],
   "source": [
    "print t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наконец, во многих фреймворках базовые слои нейронных сетей уже реализованы. Прямо как в первом домашнем задании!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 951.4027709960938)\n",
      "(1, 606.1792602539062)\n",
      "(2, 391.9593200683594)\n",
      "(3, 257.03448486328125)\n",
      "(4, 170.8016357421875)\n",
      "(5, 114.90416717529297)\n",
      "(6, 78.17976379394531)\n",
      "(7, 53.7443733215332)\n",
      "(8, 37.29325866699219)\n",
      "(9, 26.096704483032227)\n",
      "(10, 18.400386810302734)\n",
      "(11, 13.062150955200195)\n",
      "(12, 9.329204559326172)\n",
      "(13, 6.699503421783447)\n",
      "(14, 4.834671497344971)\n",
      "(15, 3.5043206214904785)\n",
      "(16, 2.550144910812378)\n",
      "(17, 1.8624448776245117)\n",
      "(18, 1.3646272420883179)\n",
      "(19, 1.0028300285339355)\n",
      "(20, 0.7389411926269531)\n",
      "(21, 0.5458341240882874)\n",
      "(22, 0.40410116314888)\n",
      "(23, 0.29979175329208374)\n",
      "(24, 0.22283178567886353)\n",
      "(25, 0.16592030227184296)\n",
      "(26, 0.1237458884716034)\n",
      "(27, 0.0924309492111206)\n",
      "(28, 0.06913767755031586)\n",
      "(29, 0.05178238824009895)\n",
      "(30, 0.03883099555969238)\n",
      "(31, 0.029152261093258858)\n",
      "(32, 0.02190937101840973)\n",
      "(33, 0.016482485458254814)\n",
      "(34, 0.012411660514771938)\n",
      "(35, 0.009354541078209877)\n",
      "(36, 0.007056237664073706)\n",
      "(37, 0.0053268191404640675)\n",
      "(38, 0.0040243836119771)\n",
      "(39, 0.0030424322467297316)\n",
      "(40, 0.0023017197381705046)\n",
      "(41, 0.0017424162942916155)\n",
      "(42, 0.0013198180822655559)\n",
      "(43, 0.001000279444269836)\n",
      "(44, 0.0007585508283227682)\n",
      "(45, 0.0005754846497438848)\n",
      "(46, 0.0004368610680103302)\n",
      "(47, 0.00033178244484588504)\n",
      "(48, 0.0002520830894354731)\n",
      "(49, 0.00019162570242770016)\n",
      "(50, 0.00014573409862350672)\n",
      "(51, 0.0001108611177187413)\n",
      "(52, 8.43774905661121e-05)\n",
      "(53, 6.423794548027217e-05)\n",
      "(54, 4.892618380836211e-05)\n",
      "(55, 3.727310468093492e-05)\n",
      "(56, 2.8407774152583443e-05)\n",
      "(57, 2.165535624953918e-05)\n",
      "(58, 1.6514257367816754e-05)\n",
      "(59, 1.2601564776559826e-05)\n",
      "(60, 9.614155715098605e-06)\n",
      "(61, 7.338019258895656e-06)\n",
      "(62, 5.603583304036874e-06)\n",
      "(63, 4.2801812014658935e-06)\n",
      "(64, 3.2691555134078953e-06)\n",
      "(65, 2.4971116090455325e-06)\n",
      "(66, 1.9092865386483027e-06)\n",
      "(67, 1.4587051282433094e-06)\n",
      "(68, 1.1165177511429647e-06)\n",
      "(69, 8.535086521987978e-07)\n",
      "(70, 6.528260882987524e-07)\n",
      "(71, 4.996422262593114e-07)\n",
      "(72, 3.821529048764205e-07)\n",
      "(73, 2.926503555045201e-07)\n",
      "(74, 2.2366178598076658e-07)\n",
      "(75, 1.7167511145999015e-07)\n",
      "(76, 1.3157070100078272e-07)\n",
      "(77, 1.0068380618122319e-07)\n",
      "(78, 7.705280324898922e-08)\n",
      "(79, 5.898216315358695e-08)\n",
      "(80, 4.521173124771849e-08)\n",
      "(81, 3.471346232686301e-08)\n",
      "(82, 2.6652134721416587e-08)\n",
      "(83, 2.0476921491763278e-08)\n",
      "(84, 1.5654840979095752e-08)\n",
      "(85, 1.2008377403560644e-08)\n",
      "(86, 9.25353038638832e-09)\n",
      "(87, 7.065581009157995e-09)\n",
      "(88, 5.430205618495165e-09)\n",
      "(89, 4.170794376534559e-09)\n",
      "(90, 3.2222728929554023e-09)\n",
      "(91, 2.4671764631278847e-09)\n",
      "(92, 1.8950698787989495e-09)\n",
      "(93, 1.464266929929181e-09)\n",
      "(94, 1.120374681917724e-09)\n",
      "(95, 8.51310133498373e-10)\n",
      "(96, 6.529574103630864e-10)\n",
      "(97, 5.12830555798871e-10)\n",
      "(98, 4.033662293068119e-10)\n",
      "(99, 3.2192626342464337e-10)\n",
      "(100, 2.4730539838202503e-10)\n",
      "(101, 1.9927073879877355e-10)\n",
      "(102, 1.6251058831961984e-10)\n",
      "(103, 1.3559589562284202e-10)\n",
      "(104, 1.1277617728566725e-10)\n",
      "(105, 9.595002570250699e-11)\n",
      "(106, 8.175940480192878e-11)\n",
      "(107, 6.97309987529593e-11)\n",
      "(108, 5.7742668285731824e-11)\n",
      "(109, 5.147384601889016e-11)\n",
      "(110, 4.5765991690815966e-11)\n",
      "(111, 4.0691464087139195e-11)\n",
      "(112, 3.6548028492511264e-11)\n",
      "(113, 3.47974878078805e-11)\n",
      "(114, 3.3315850078707854e-11)\n",
      "(115, 2.9433087911368006e-11)\n",
      "(116, 2.5687778293370656e-11)\n",
      "(117, 2.409802565828123e-11)\n",
      "(118, 2.2337076632794606e-11)\n",
      "(119, 2.1009510103864315e-11)\n",
      "(120, 1.9104450660334393e-11)\n",
      "(121, 1.7720562864309564e-11)\n",
      "(122, 1.6829660928752155e-11)\n",
      "(123, 1.5376472664585528e-11)\n",
      "(124, 1.465270439704458e-11)\n",
      "(125, 1.398492606441426e-11)\n",
      "(126, 1.2789659956102817e-11)\n",
      "(127, 1.2659763862221673e-11)\n",
      "(128, 1.2099753490812937e-11)\n",
      "(129, 1.1685681935991177e-11)\n",
      "(130, 1.0685322418546583e-11)\n",
      "(131, 9.908083034582127e-12)\n",
      "(132, 9.878453957612443e-12)\n",
      "(133, 9.556336624805262e-12)\n",
      "(134, 9.016310267839778e-12)\n",
      "(135, 8.462714903845203e-12)\n",
      "(136, 8.239309408353268e-12)\n",
      "(137, 8.178052852969575e-12)\n",
      "(138, 7.636139116862228e-12)\n",
      "(139, 7.77753989683605e-12)\n",
      "(140, 7.526116015121875e-12)\n",
      "(141, 7.601697916970185e-12)\n",
      "(142, 7.375906309337044e-12)\n",
      "(143, 6.947152055014616e-12)\n",
      "(144, 6.724913161060275e-12)\n",
      "(145, 6.685666777139776e-12)\n",
      "(146, 6.885864274608355e-12)\n",
      "(147, 6.671549597492277e-12)\n",
      "(148, 6.175723994694682e-12)\n",
      "(149, 6.334624665094157e-12)\n",
      "(150, 5.857978165046962e-12)\n",
      "(151, 5.951639354961902e-12)\n",
      "(152, 5.559397560361834e-12)\n",
      "(153, 5.324599268441421e-12)\n",
      "(154, 5.206235616228572e-12)\n",
      "(155, 5.121123143603246e-12)\n",
      "(156, 5.204861715235598e-12)\n",
      "(157, 5.290973388583087e-12)\n",
      "(158, 4.761924361773584e-12)\n",
      "(159, 4.8517969156169904e-12)\n",
      "(160, 4.684514061381595e-12)\n",
      "(161, 4.970382612434765e-12)\n",
      "(162, 5.0473765791925196e-12)\n",
      "(163, 5.117487163197598e-12)\n",
      "(164, 5.007533450396284e-12)\n",
      "(165, 4.958558737222507e-12)\n",
      "(166, 4.825456874357759e-12)\n",
      "(167, 4.534869875449932e-12)\n",
      "(168, 4.351988387718553e-12)\n",
      "(169, 4.415520900302727e-12)\n",
      "(170, 4.465564203137706e-12)\n",
      "(171, 4.746617161821565e-12)\n",
      "(172, 4.641284752360253e-12)\n",
      "(173, 4.767253432291785e-12)\n",
      "(174, 4.514191971616288e-12)\n",
      "(175, 4.62085664870715e-12)\n",
      "(176, 4.452824393930133e-12)\n",
      "(177, 4.633374413309799e-12)\n",
      "(178, 4.52453092353311e-12)\n",
      "(179, 4.484493505707565e-12)\n",
      "(180, 4.404175808769839e-12)\n",
      "(181, 4.0879842913565945e-12)\n",
      "(182, 4.162716178701675e-12)\n",
      "(183, 3.973117841671314e-12)\n",
      "(184, 3.909474306784677e-12)\n",
      "(185, 3.6690555108020995e-12)\n",
      "(186, 3.773555252994942e-12)\n",
      "(187, 3.834284452441938e-12)\n",
      "(188, 3.7142277101165355e-12)\n",
      "(189, 3.761328921936258e-12)\n",
      "(190, 3.6402591011008845e-12)\n",
      "(191, 3.577753544814488e-12)\n",
      "(192, 3.863205762233424e-12)\n",
      "(193, 3.768434349293859e-12)\n",
      "(194, 3.5595597649984434e-12)\n",
      "(195, 3.589730075692632e-12)\n",
      "(196, 3.853297021738644e-12)\n",
      "(197, 3.784345233015518e-12)\n",
      "(198, 3.778294517531311e-12)\n",
      "(199, 3.7345040254654904e-12)\n",
      "(200, 3.6006805176347356e-12)\n",
      "(201, 3.3721966191668784e-12)\n",
      "(202, 3.571176340755322e-12)\n",
      "(203, 3.631392062053429e-12)\n",
      "(204, 3.5408672521830553e-12)\n",
      "(205, 3.2206650540933524e-12)\n",
      "(206, 3.2875282357514024e-12)\n",
      "(207, 3.196545458883371e-12)\n",
      "(208, 3.378733057224359e-12)\n",
      "(209, 3.471034223934133e-12)\n",
      "(210, 3.4958268918527935e-12)\n",
      "(211, 3.378670607179224e-12)\n",
      "(212, 3.5495339306690354e-12)\n",
      "(213, 3.881490615031957e-12)\n",
      "(214, 3.938250767165918e-12)\n",
      "(215, 3.753162711173097e-12)\n",
      "(216, 3.775727994148603e-12)\n",
      "(217, 3.779086418798094e-12)\n",
      "(218, 3.71313917113536e-12)\n",
      "(219, 3.80933999621913e-12)\n",
      "(220, 3.720799710005274e-12)\n",
      "(221, 3.681719859538468e-12)\n",
      "(222, 3.513139432143042e-12)\n",
      "(223, 3.547556345906422e-12)\n",
      "(224, 3.776276166767012e-12)\n",
      "(225, 3.638743820144619e-12)\n",
      "(226, 3.4614412031119812e-12)\n",
      "(227, 3.491986214076981e-12)\n",
      "(228, 3.5082232258121238e-12)\n",
      "(229, 3.430119036029744e-12)\n",
      "(230, 3.2975306513138847e-12)\n",
      "(231, 3.3587316955463464e-12)\n",
      "(232, 3.3296299745133595e-12)\n",
      "(233, 3.5300668638216237e-12)\n",
      "(234, 3.4498116169290327e-12)\n",
      "(235, 3.337970524985856e-12)\n",
      "(236, 3.239846758928966e-12)\n",
      "(237, 3.2857267254216005e-12)\n",
      "(238, 3.2461056412302902e-12)\n",
      "(239, 3.1764391464350616e-12)\n",
      "(240, 3.1107833323162914e-12)\n",
      "(241, 3.014027395720209e-12)\n",
      "(242, 2.8240682362068448e-12)\n",
      "(243, 2.909777453707907e-12)\n",
      "(244, 2.8511854335833142e-12)\n",
      "(245, 2.8741115390418237e-12)\n",
      "(246, 2.809760236976988e-12)\n",
      "(247, 2.8976205115882614e-12)\n",
      "(248, 2.754790319470235e-12)\n",
      "(249, 2.805666289573683e-12)\n",
      "(250, 2.7801311600073042e-12)\n",
      "(251, 2.8575275826114854e-12)\n",
      "(252, 2.9617775246237876e-12)\n",
      "(253, 3.0822228450078093e-12)\n",
      "(254, 3.3328279372413228e-12)\n",
      "(255, 3.4580610944190404e-12)\n",
      "(256, 3.333410804329251e-12)\n",
      "(257, 3.2468272861962966e-12)\n",
      "(258, 3.2262742824529234e-12)\n",
      "(259, 3.433414143272362e-12)\n",
      "(260, 3.3914962851988584e-12)\n",
      "(261, 3.321746523676783e-12)\n",
      "(262, 3.2563960208897846e-12)\n",
      "(263, 3.032963637183972e-12)\n",
      "(264, 3.1311351081364514e-12)\n",
      "(265, 3.266027205628408e-12)\n",
      "(266, 3.27020441975856e-12)\n",
      "(267, 3.213805090107602e-12)\n",
      "(268, 3.2164696253667024e-12)\n",
      "(269, 3.229431479179201e-12)\n",
      "(270, 3.219481105320998e-12)\n",
      "(271, 2.8910355012734534e-12)\n",
      "(272, 2.7666627669398203e-12)\n",
      "(273, 2.9576766383265785e-12)\n",
      "(274, 3.082771017626218e-12)\n",
      "(275, 3.035503272352802e-12)\n",
      "(276, 2.9987271346620936e-12)\n",
      "(277, 2.8897587447951345e-12)\n",
      "(278, 2.9995667408244664e-12)\n",
      "(279, 3.0101138595584054e-12)\n",
      "(280, 3.092708381058351e-12)\n",
      "(281, 3.0381478582919286e-12)\n",
      "(282, 3.125106076695694e-12)\n",
      "(283, 3.0862621486216213e-12)\n",
      "(284, 2.9503292170440787e-12)\n",
      "(285, 2.7495870164040426e-12)\n",
      "(286, 2.6836189520595966e-12)\n",
      "(287, 2.513935240533449e-12)\n",
      "(288, 2.650825739469731e-12)\n",
      "(289, 2.4959635053223295e-12)\n",
      "(290, 2.671017920730101e-12)\n",
      "(291, 2.4235492085411536e-12)\n",
      "(292, 2.5230529471231833e-12)\n",
      "(293, 2.4247426982926257e-12)\n",
      "(294, 2.3773361751411315e-12)\n",
      "(295, 2.267132662159277e-12)\n",
      "(296, 2.4984077306999808e-12)\n",
      "(297, 2.4279085686362833e-12)\n",
      "(298, 2.327627673937016e-12)\n",
      "(299, 2.2383657427571535e-12)\n",
      "(300, 2.192777209808483e-12)\n",
      "(301, 2.202047572064103e-12)\n",
      "(302, 2.290449080399881e-12)\n",
      "(303, 2.3613229427343896e-12)\n",
      "(304, 2.5370296141691284e-12)\n",
      "(305, 2.5469105990882923e-12)\n",
      "(306, 2.680602267934873e-12)\n",
      "(307, 2.8056133805076655e-12)\n",
      "(308, 2.830551765198308e-12)\n",
      "(309, 2.661908887757747e-12)\n",
      "(310, 2.5141382031801385e-12)\n",
      "(311, 2.5112099899526896e-12)\n",
      "(312, 2.6248274387352666e-12)\n",
      "(313, 2.501315127245718e-12)\n",
      "(314, 2.636318247040137e-12)\n",
      "(315, 2.7070116981331438e-12)\n",
      "(316, 2.6092010496636675e-12)\n",
      "(317, 2.5904105249718867e-12)\n",
      "(318, 2.556215655813432e-12)\n",
      "(319, 2.476612664947808e-12)\n",
      "(320, 2.4067380033354624e-12)\n",
      "(321, 2.4830103251272106e-12)\n",
      "(322, 2.4778408491687998e-12)\n",
      "(323, 2.3533501536388e-12)\n",
      "(324, 2.5285849802880733e-12)\n",
      "(325, 2.511543056860077e-12)\n",
      "(326, 2.5587275354066463e-12)\n",
      "(327, 2.1692474205803336e-12)\n",
      "(328, 2.2084382933496016e-12)\n",
      "(329, 2.3208483745928987e-12)\n",
      "(330, 2.527627412929334e-12)\n",
      "(331, 2.3303824148168673e-12)\n",
      "(332, 2.4232456319328577e-12)\n",
      "(333, 2.339978905085971e-12)\n",
      "(334, 2.1539194039466025e-12)\n",
      "(335, 2.146244987288881e-12)\n",
      "(336, 2.070270170573263e-12)\n",
      "(337, 2.1611835585022554e-12)\n",
      "(338, 2.366005828757789e-12)\n",
      "(339, 2.4729203060291916e-12)\n",
      "(340, 2.407750214483695e-12)\n",
      "(341, 2.190146501657164e-12)\n",
      "(342, 2.2773545202414702e-12)\n",
      "(343, 2.346937748309852e-12)\n",
      "(344, 2.285375881594387e-12)\n",
      "(345, 2.3194458506625715e-12)\n",
      "(346, 2.3896119458188814e-12)\n",
      "(347, 2.6668901462190142e-12)\n",
      "(348, 2.784788892540302e-12)\n",
      "(349, 2.851423958061261e-12)\n",
      "(350, 2.666682846763635e-12)\n",
      "(351, 2.522711206598416e-12)\n",
      "(352, 2.584717162523731e-12)\n",
      "(353, 2.512830221679252e-12)\n",
      "(354, 2.562790257787384e-12)\n",
      "(355, 2.4572080481455316e-12)\n",
      "(356, 2.4560423139696752e-12)\n",
      "(357, 2.4266352816049164e-12)\n",
      "(358, 2.404434290559365e-12)\n",
      "(359, 2.4113107344181373e-12)\n",
      "(360, 2.4333625392447544e-12)\n",
      "(361, 2.3397707382688537e-12)\n",
      "(362, 2.196954423938635e-12)\n",
      "(363, 2.195982978792088e-12)\n",
      "(364, 2.2869657556601197e-12)\n",
      "(365, 2.1261777061187814e-12)\n",
      "(366, 2.3833747475610068e-12)\n",
      "(367, 2.4297126810512992e-12)\n",
      "(368, 2.420470074371295e-12)\n",
      "(369, 2.448128505472269e-12)\n",
      "(370, 2.1167269326216598e-12)\n",
      "(371, 2.1314928988491744e-12)\n",
      "(372, 2.2129277577054296e-12)\n",
      "(373, 2.067044452269684e-12)\n",
      "(374, 2.0559977331746637e-12)\n",
      "(375, 1.8931002598865376e-12)\n",
      "(376, 1.890518991354284e-12)\n",
      "(377, 2.01214392370197e-12)\n",
      "(378, 2.050002528841688e-12)\n",
      "(379, 2.0393443878052864e-12)\n",
      "(380, 2.0847525095124553e-12)\n",
      "(381, 2.030462603608285e-12)\n",
      "(382, 1.886633210768096e-12)\n",
      "(383, 2.0293107472202365e-12)\n",
      "(384, 2.245873625961181e-12)\n",
      "(385, 2.181328034867036e-12)\n",
      "(386, 2.0669195521794137e-12)\n",
      "(387, 2.072803734209927e-12)\n",
      "(388, 2.1416653173123024e-12)\n",
      "(389, 2.003331528444008e-12)\n",
      "(390, 2.1380848480578862e-12)\n",
      "(391, 2.0730812899660833e-12)\n",
      "(392, 2.20047938204182e-12)\n",
      "(393, 2.203934951205966e-12)\n",
      "(394, 2.297263074213518e-12)\n",
      "(395, 2.2382963538181144e-12)\n",
      "(396, 2.4280473465143615e-12)\n",
      "(397, 2.352760347656968e-12)\n",
      "(398, 2.43161393798097e-12)\n",
      "(399, 2.5294523420260617e-12)\n",
      "(400, 2.449252606284702e-12)\n",
      "(401, 2.4514730523339523e-12)\n",
      "(402, 2.381348590541066e-12)\n",
      "(403, 2.232578705241295e-12)\n",
      "(404, 2.1425396179441947e-12)\n",
      "(405, 2.2012704159468655e-12)\n",
      "(406, 2.1942482553161113e-12)\n",
      "(407, 2.239600865872049e-12)\n",
      "(408, 2.240183732959977e-12)\n",
      "(409, 2.2367420415836392e-12)\n",
      "(410, 2.2508401392729027e-12)\n",
      "(411, 2.3724720105144925e-12)\n",
      "(412, 2.2004568306366323e-12)\n",
      "(413, 2.221786990497243e-12)\n",
      "(414, 2.337139162755797e-12)\n",
      "(415, 2.364797593856771e-12)\n",
      "(416, 2.4421662608853367e-12)\n",
      "(417, 2.281863933917272e-12)\n",
      "(418, 2.1157051804943094e-12)\n",
      "(419, 2.1474159256351655e-12)\n",
      "(420, 2.2077149136601193e-12)\n",
      "(421, 2.169821614050882e-12)\n",
      "(422, 2.3572966495466474e-12)\n",
      "(423, 2.2131480675868787e-12)\n",
      "(424, 2.240487309568273e-12)\n",
      "(425, 2.3646865715543086e-12)\n",
      "(426, 2.5642491602306805e-12)\n",
      "(427, 2.532580048453248e-12)\n",
      "(428, 2.0970673114684146e-12)\n",
      "(429, 2.2562247209423347e-12)\n",
      "(430, 2.4504027279492746e-12)\n",
      "(431, 2.402441093285468e-12)\n",
      "(432, 2.5265640274385603e-12)\n",
      "(433, 2.2450253461814285e-12)\n",
      "(434, 2.2718649878017416e-12)\n",
      "(435, 2.218962860678353e-12)\n",
      "(436, 2.1654986831487477e-12)\n",
      "(437, 2.1428501334463945e-12)\n",
      "(438, 2.0452337740062276e-12)\n",
      "(439, 1.97714934702109e-12)\n",
      "(440, 1.951128494881438e-12)\n",
      "(441, 2.011621771935701e-12)\n",
      "(442, 2.0117327942381635e-12)\n",
      "(443, 1.95800493874021e-12)\n",
      "(444, 2.1432040170354938e-12)\n",
      "(445, 2.1038882441759554e-12)\n",
      "(446, 2.1930391530533555e-12)\n",
      "(447, 2.1881680495328126e-12)\n",
      "(448, 2.0418961660384483e-12)\n",
      "(449, 2.081322961200449e-12)\n",
      "(450, 2.0676949735731753e-12)\n",
      "(451, 2.0904198511084715e-12)\n",
      "(452, 1.969891263997603e-12)\n",
      "(453, 2.1266790412033387e-12)\n",
      "(454, 2.2029929963585104e-12)\n",
      "(455, 2.0065529099388968e-12)\n",
      "(456, 2.132105256236194e-12)\n",
      "(457, 1.9735792861075296e-12)\n",
      "(458, 2.1728504412399374e-12)\n",
      "(459, 2.119622186103065e-12)\n",
      "(460, 2.1209544537326153e-12)\n",
      "(461, 2.219042657958248e-12)\n",
      "(462, 2.4010637228455423e-12)\n",
      "(463, 2.400702900362539e-12)\n",
      "(464, 2.1848200332241774e-12)\n",
      "(465, 2.2726386744720273e-12)\n",
      "(466, 2.285600528284526e-12)\n",
      "(467, 2.499873572037181e-12)\n",
      "(468, 2.5164436506797117e-12)\n",
      "(469, 2.510517835285775e-12)\n",
      "(470, 2.4672607706888172e-12)\n",
      "(471, 2.436007992545619e-12)\n",
      "(472, 2.3576678803705065e-12)\n",
      "(473, 2.635417925556105e-12)\n",
      "(474, 2.7209328540278577e-12)\n",
      "(475, 2.762122128241451e-12)\n",
      "(476, 2.5549545118463968e-12)\n",
      "(477, 2.716186650597585e-12)\n",
      "(478, 2.7394319451756743e-12)\n",
      "(479, 2.6226087274094922e-12)\n",
      "(480, 2.513140737181452e-12)\n",
      "(481, 2.459586354031096e-12)\n",
      "(482, 2.3445672486799296e-12)\n",
      "(483, 2.444084865049767e-12)\n",
      "(484, 2.596962575540651e-12)\n",
      "(485, 2.5068957326679353e-12)\n",
      "(486, 2.353879244298973e-12)\n",
      "(487, 2.321127665072531e-12)\n",
      "(488, 2.0870371403303167e-12)\n",
      "(489, 2.17908850685955e-12)\n",
      "(490, 2.296300302684351e-12)\n",
      "(491, 2.267420626256289e-12)\n",
      "(492, 2.30564005387901e-12)\n",
      "(493, 2.3517559427643775e-12)\n",
      "(494, 2.30640333220844e-12)\n",
      "(495, 2.4918660884720722e-12)\n",
      "(496, 2.4714379848189694e-12)\n",
      "(497, 2.2898054979902938e-12)\n",
      "(498, 2.3994955328232592e-12)\n",
      "(499, 2.4969176032341167e-12)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "N, D_in, D_out = 64, 1000, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, D_out)\n",
    "        )\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-12 *\n",
       "  2.4930\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А еще нам есть уже готовые оптимизаторы, такие как GD, SGD, ADAM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 929.259521484375)\n",
      "(1, 913.812744140625)\n",
      "(2, 898.5112915039062)\n",
      "(3, 883.359130859375)\n",
      "(4, 868.3568725585938)\n",
      "(5, 853.5076904296875)\n",
      "(6, 838.812744140625)\n",
      "(7, 824.2747802734375)\n",
      "(8, 809.8948364257812)\n",
      "(9, 795.6734619140625)\n",
      "(10, 781.6139526367188)\n",
      "(11, 767.7159423828125)\n",
      "(12, 753.981689453125)\n",
      "(13, 740.4117431640625)\n",
      "(14, 727.0078125)\n",
      "(15, 713.7693481445312)\n",
      "(16, 700.6981201171875)\n",
      "(17, 687.7933959960938)\n",
      "(18, 675.0564575195312)\n",
      "(19, 662.4876708984375)\n",
      "(20, 650.0873413085938)\n",
      "(21, 637.8545532226562)\n",
      "(22, 625.7894897460938)\n",
      "(23, 613.8923950195312)\n",
      "(24, 602.162841796875)\n",
      "(25, 590.6002197265625)\n",
      "(26, 579.2041015625)\n",
      "(27, 567.9739379882812)\n",
      "(28, 556.9091796875)\n",
      "(29, 546.0087280273438)\n",
      "(30, 535.2716064453125)\n",
      "(31, 524.697265625)\n",
      "(32, 514.2847900390625)\n",
      "(33, 504.0326843261719)\n",
      "(34, 493.9402160644531)\n",
      "(35, 484.0062561035156)\n",
      "(36, 474.22906494140625)\n",
      "(37, 464.6074523925781)\n",
      "(38, 455.140380859375)\n",
      "(39, 445.8262634277344)\n",
      "(40, 436.6641845703125)\n",
      "(41, 427.6524353027344)\n",
      "(42, 418.7889709472656)\n",
      "(43, 410.0730285644531)\n",
      "(44, 401.5025634765625)\n",
      "(45, 393.076904296875)\n",
      "(46, 384.79364013671875)\n",
      "(47, 376.6515197753906)\n",
      "(48, 368.6490478515625)\n",
      "(49, 360.784423828125)\n",
      "(50, 353.056640625)\n",
      "(51, 345.4629211425781)\n",
      "(52, 338.0028991699219)\n",
      "(53, 330.6744079589844)\n",
      "(54, 323.4755554199219)\n",
      "(55, 316.4051513671875)\n",
      "(56, 309.4615783691406)\n",
      "(57, 302.6430969238281)\n",
      "(58, 295.9479064941406)\n",
      "(59, 289.37481689453125)\n",
      "(60, 282.9218444824219)\n",
      "(61, 276.587646484375)\n",
      "(62, 270.37054443359375)\n",
      "(63, 264.2688903808594)\n",
      "(64, 258.28118896484375)\n",
      "(65, 252.40599060058594)\n",
      "(66, 246.64134216308594)\n",
      "(67, 240.98622131347656)\n",
      "(68, 235.43882751464844)\n",
      "(69, 229.9974365234375)\n",
      "(70, 224.6609344482422)\n",
      "(71, 219.4273681640625)\n",
      "(72, 214.295654296875)\n",
      "(73, 209.26412963867188)\n",
      "(74, 204.3311309814453)\n",
      "(75, 199.4954376220703)\n",
      "(76, 194.7555389404297)\n",
      "(77, 190.1100311279297)\n",
      "(78, 185.5574951171875)\n",
      "(79, 181.09637451171875)\n",
      "(80, 176.7253875732422)\n",
      "(81, 172.44308471679688)\n",
      "(82, 168.24806213378906)\n",
      "(83, 164.1389617919922)\n",
      "(84, 160.11453247070312)\n",
      "(85, 156.17337036132812)\n",
      "(86, 152.31423950195312)\n",
      "(87, 148.5355224609375)\n",
      "(88, 144.83624267578125)\n",
      "(89, 141.21484375)\n",
      "(90, 137.6703338623047)\n",
      "(91, 134.20130920410156)\n",
      "(92, 130.8064727783203)\n",
      "(93, 127.4845199584961)\n",
      "(94, 124.23448944091797)\n",
      "(95, 121.05488586425781)\n",
      "(96, 117.94464874267578)\n",
      "(97, 114.90261840820312)\n",
      "(98, 111.92760467529297)\n",
      "(99, 109.01839447021484)\n",
      "(100, 106.17378234863281)\n",
      "(101, 103.3927993774414)\n",
      "(102, 100.67414855957031)\n",
      "(103, 98.01683044433594)\n",
      "(104, 95.4197998046875)\n",
      "(105, 92.8818130493164)\n",
      "(106, 90.40189361572266)\n",
      "(107, 87.97900390625)\n",
      "(108, 85.61200714111328)\n",
      "(109, 83.29998779296875)\n",
      "(110, 81.0417709350586)\n",
      "(111, 78.8365249633789)\n",
      "(112, 76.68314361572266)\n",
      "(113, 74.5806884765625)\n",
      "(114, 72.52812957763672)\n",
      "(115, 70.5245590209961)\n",
      "(116, 68.56906127929688)\n",
      "(117, 66.66068267822266)\n",
      "(118, 64.79838562011719)\n",
      "(119, 62.981380462646484)\n",
      "(120, 61.20876693725586)\n",
      "(121, 59.47970199584961)\n",
      "(122, 57.793155670166016)\n",
      "(123, 56.148475646972656)\n",
      "(124, 54.54465103149414)\n",
      "(125, 52.98091506958008)\n",
      "(126, 51.45643997192383)\n",
      "(127, 49.970394134521484)\n",
      "(128, 48.5219841003418)\n",
      "(129, 47.11046600341797)\n",
      "(130, 45.73501205444336)\n",
      "(131, 44.394859313964844)\n",
      "(132, 43.089290618896484)\n",
      "(133, 41.817543029785156)\n",
      "(134, 40.57890319824219)\n",
      "(135, 39.372615814208984)\n",
      "(136, 38.19801330566406)\n",
      "(137, 37.054386138916016)\n",
      "(138, 35.94109344482422)\n",
      "(139, 34.85737228393555)\n",
      "(140, 33.80261993408203)\n",
      "(141, 32.77619171142578)\n",
      "(142, 31.77739906311035)\n",
      "(143, 30.80567169189453)\n",
      "(144, 29.860353469848633)\n",
      "(145, 28.940860748291016)\n",
      "(146, 28.046567916870117)\n",
      "(147, 27.176931381225586)\n",
      "(148, 26.33133316040039)\n",
      "(149, 25.509206771850586)\n",
      "(150, 24.71004295349121)\n",
      "(151, 23.933244705200195)\n",
      "(152, 23.178319931030273)\n",
      "(153, 22.444698333740234)\n",
      "(154, 21.731910705566406)\n",
      "(155, 21.03940773010254)\n",
      "(156, 20.366689682006836)\n",
      "(157, 19.713359832763672)\n",
      "(158, 19.07881736755371)\n",
      "(159, 18.46265983581543)\n",
      "(160, 17.864421844482422)\n",
      "(161, 17.283655166625977)\n",
      "(162, 16.719890594482422)\n",
      "(163, 16.172727584838867)\n",
      "(164, 15.641733169555664)\n",
      "(165, 15.126493453979492)\n",
      "(166, 14.626608848571777)\n",
      "(167, 14.141646385192871)\n",
      "(168, 13.67125415802002)\n",
      "(169, 13.215052604675293)\n",
      "(170, 12.772645950317383)\n",
      "(171, 12.343672752380371)\n",
      "(172, 11.927789688110352)\n",
      "(173, 11.524629592895508)\n",
      "(174, 11.133859634399414)\n",
      "(175, 10.755146026611328)\n",
      "(176, 10.388157844543457)\n",
      "(177, 10.032583236694336)\n",
      "(178, 9.688098907470703)\n",
      "(179, 9.354401588439941)\n",
      "(180, 9.031188011169434)\n",
      "(181, 8.718192100524902)\n",
      "(182, 8.415091514587402)\n",
      "(183, 8.121637344360352)\n",
      "(184, 7.837554454803467)\n",
      "(185, 7.56254768371582)\n",
      "(186, 7.296392917633057)\n",
      "(187, 7.038825035095215)\n",
      "(188, 6.78959321975708)\n",
      "(189, 6.548456192016602)\n",
      "(190, 6.315189838409424)\n",
      "(191, 6.0895562171936035)\n",
      "(192, 5.871333122253418)\n",
      "(193, 5.660303115844727)\n",
      "(194, 5.456253528594971)\n",
      "(195, 5.2589826583862305)\n",
      "(196, 5.068274021148682)\n",
      "(197, 4.883947372436523)\n",
      "(198, 4.705799102783203)\n",
      "(199, 4.533651351928711)\n",
      "(200, 4.367320537567139)\n",
      "(201, 4.206625461578369)\n",
      "(202, 4.051388263702393)\n",
      "(203, 3.9014599323272705)\n",
      "(204, 3.7566604614257812)\n",
      "(205, 3.616835355758667)\n",
      "(206, 3.4818320274353027)\n",
      "(207, 3.3515007495880127)\n",
      "(208, 3.22568678855896)\n",
      "(209, 3.104257583618164)\n",
      "(210, 2.9870691299438477)\n",
      "(211, 2.873988151550293)\n",
      "(212, 2.7648842334747314)\n",
      "(213, 2.6596310138702393)\n",
      "(214, 2.5580995082855225)\n",
      "(215, 2.4601757526397705)\n",
      "(216, 2.365739345550537)\n",
      "(217, 2.2746779918670654)\n",
      "(218, 2.1868820190429688)\n",
      "(219, 2.1022439002990723)\n",
      "(220, 2.0206592082977295)\n",
      "(221, 1.942026972770691)\n",
      "(222, 1.8662505149841309)\n",
      "(223, 1.7932343482971191)\n",
      "(224, 1.722885012626648)\n",
      "(225, 1.6551142930984497)\n",
      "(226, 1.5898357629776)\n",
      "(227, 1.526963233947754)\n",
      "(228, 1.4664194583892822)\n",
      "(229, 1.4081188440322876)\n",
      "(230, 1.3519904613494873)\n",
      "(231, 1.297955870628357)\n",
      "(232, 1.2459447383880615)\n",
      "(233, 1.1958879232406616)\n",
      "(234, 1.1477148532867432)\n",
      "(235, 1.1013643741607666)\n",
      "(236, 1.056767463684082)\n",
      "(237, 1.0138665437698364)\n",
      "(238, 0.9726006984710693)\n",
      "(239, 0.9329137206077576)\n",
      "(240, 0.8947480320930481)\n",
      "(241, 0.8580500483512878)\n",
      "(242, 0.8227658867835999)\n",
      "(243, 0.7888487577438354)\n",
      "(244, 0.7562459707260132)\n",
      "(245, 0.7249116897583008)\n",
      "(246, 0.6948003172874451)\n",
      "(247, 0.6658661365509033)\n",
      "(248, 0.6380681395530701)\n",
      "(249, 0.6113633513450623)\n",
      "(250, 0.5857131481170654)\n",
      "(251, 0.5610771775245667)\n",
      "(252, 0.5374183058738708)\n",
      "(253, 0.5147015452384949)\n",
      "(254, 0.49289196729660034)\n",
      "(255, 0.4719536006450653)\n",
      "(256, 0.4518560767173767)\n",
      "(257, 0.43256834149360657)\n",
      "(258, 0.4140574336051941)\n",
      "(259, 0.39629608392715454)\n",
      "(260, 0.3792554438114166)\n",
      "(261, 0.3629077076911926)\n",
      "(262, 0.34722721576690674)\n",
      "(263, 0.33218657970428467)\n",
      "(264, 0.3177637755870819)\n",
      "(265, 0.3039340376853943)\n",
      "(266, 0.2906738221645355)\n",
      "(267, 0.2779623568058014)\n",
      "(268, 0.26577749848365784)\n",
      "(269, 0.2540995180606842)\n",
      "(270, 0.24290770292282104)\n",
      "(271, 0.23218384385108948)\n",
      "(272, 0.2219090610742569)\n",
      "(273, 0.2120661437511444)\n",
      "(274, 0.20263683795928955)\n",
      "(275, 0.1936061680316925)\n",
      "(276, 0.18495765328407288)\n",
      "(277, 0.17667603492736816)\n",
      "(278, 0.16874702274799347)\n",
      "(279, 0.16115610301494598)\n",
      "(280, 0.15388968586921692)\n",
      "(281, 0.14693515002727509)\n",
      "(282, 0.1402788758277893)\n",
      "(283, 0.13391053676605225)\n",
      "(284, 0.12781661748886108)\n",
      "(285, 0.12198661267757416)\n",
      "(286, 0.11640996485948563)\n",
      "(287, 0.11107629537582397)\n",
      "(288, 0.10597498714923859)\n",
      "(289, 0.10109687596559525)\n",
      "(290, 0.0964328721165657)\n",
      "(291, 0.09197390824556351)\n",
      "(292, 0.08771156519651413)\n",
      "(293, 0.08363749831914902)\n",
      "(294, 0.07974355667829514)\n",
      "(295, 0.07602299749851227)\n",
      "(296, 0.07246777415275574)\n",
      "(297, 0.06907135248184204)\n",
      "(298, 0.0658266469836235)\n",
      "(299, 0.06272759288549423)\n",
      "(300, 0.05976785719394684)\n",
      "(301, 0.05694149434566498)\n",
      "(302, 0.05424267798662186)\n",
      "(303, 0.05166623368859291)\n",
      "(304, 0.04920662194490433)\n",
      "(305, 0.046858858317136765)\n",
      "(306, 0.044618334621191025)\n",
      "(307, 0.04248002916574478)\n",
      "(308, 0.04043989256024361)\n",
      "(309, 0.03849343582987785)\n",
      "(310, 0.03663627430796623)\n",
      "(311, 0.034865282475948334)\n",
      "(312, 0.033175915479660034)\n",
      "(313, 0.031565044075250626)\n",
      "(314, 0.030028952285647392)\n",
      "(315, 0.02856447920203209)\n",
      "(316, 0.027168450877070427)\n",
      "(317, 0.025837641209363937)\n",
      "(318, 0.02456923946738243)\n",
      "(319, 0.023360660299658775)\n",
      "(320, 0.022208919748663902)\n",
      "(321, 0.021111613139510155)\n",
      "(322, 0.020066266879439354)\n",
      "(323, 0.019070550799369812)\n",
      "(324, 0.018122168257832527)\n",
      "(325, 0.01721901446580887)\n",
      "(326, 0.01635909453034401)\n",
      "(327, 0.015540331602096558)\n",
      "(328, 0.014760799705982208)\n",
      "(329, 0.014018978923559189)\n",
      "(330, 0.01331261731684208)\n",
      "(331, 0.012640709988772869)\n",
      "(332, 0.012001240625977516)\n",
      "(333, 0.01139280665665865)\n",
      "(334, 0.010814148932695389)\n",
      "(335, 0.010263506323099136)\n",
      "(336, 0.009739842265844345)\n",
      "(337, 0.009241894818842411)\n",
      "(338, 0.008768310770392418)\n",
      "(339, 0.008318260312080383)\n",
      "(340, 0.007890126667916775)\n",
      "(341, 0.007483382243663073)\n",
      "(342, 0.007096627727150917)\n",
      "(343, 0.006729218177497387)\n",
      "(344, 0.006380013655871153)\n",
      "(345, 0.006048298440873623)\n",
      "(346, 0.005733177997171879)\n",
      "(347, 0.005433723796159029)\n",
      "(348, 0.005149421747773886)\n",
      "(349, 0.00487942062318325)\n",
      "(350, 0.004622989799827337)\n",
      "(351, 0.004379611928015947)\n",
      "(352, 0.004148490261286497)\n",
      "(353, 0.003929153550416231)\n",
      "(354, 0.0037209128495305777)\n",
      "(355, 0.0035233914386481047)\n",
      "(356, 0.0033359185326844454)\n",
      "(357, 0.0031580959912389517)\n",
      "(358, 0.002989279106259346)\n",
      "(359, 0.0028291894122958183)\n",
      "(360, 0.002677411073818803)\n",
      "(361, 0.002533514052629471)\n",
      "(362, 0.002397014992311597)\n",
      "(363, 0.0022676195949316025)\n",
      "(364, 0.002144919941201806)\n",
      "(365, 0.0020286529324948788)\n",
      "(366, 0.0019184356788173318)\n",
      "(367, 0.0018140011234208941)\n",
      "(368, 0.0017150769708678126)\n",
      "(369, 0.0016213132767006755)\n",
      "(370, 0.0015324942069128156)\n",
      "(371, 0.0014483891427516937)\n",
      "(372, 0.0013687243917956948)\n",
      "(373, 0.0012932606041431427)\n",
      "(374, 0.0012218826450407505)\n",
      "(375, 0.0011542123975232244)\n",
      "(376, 0.0010902106296271086)\n",
      "(377, 0.0010296216933056712)\n",
      "(378, 0.000972289068158716)\n",
      "(379, 0.0009180324850603938)\n",
      "(380, 0.0008667054935358465)\n",
      "(381, 0.0008181233424693346)\n",
      "(382, 0.000772189989220351)\n",
      "(383, 0.0007287489133886993)\n",
      "(384, 0.0006876476691104472)\n",
      "(385, 0.0006487926002591848)\n",
      "(386, 0.0006120768957771361)\n",
      "(387, 0.0005773472366854548)\n",
      "(388, 0.0005445530987344682)\n",
      "(389, 0.0005135288811288774)\n",
      "(390, 0.00048419900122098625)\n",
      "(391, 0.0004565073468256742)\n",
      "(392, 0.00043036736315116286)\n",
      "(393, 0.00040564671508036554)\n",
      "(394, 0.00038230381323955953)\n",
      "(395, 0.00036025678855367005)\n",
      "(396, 0.00033944720053113997)\n",
      "(397, 0.00031979416962713003)\n",
      "(398, 0.00030123002943582833)\n",
      "(399, 0.0002836989297065884)\n",
      "(400, 0.0002671841357368976)\n",
      "(401, 0.0002515764790587127)\n",
      "(402, 0.00023686460917815566)\n",
      "(403, 0.00022297938994597644)\n",
      "(404, 0.00020988061442039907)\n",
      "(405, 0.00019752801745198667)\n",
      "(406, 0.00018587066733743995)\n",
      "(407, 0.00017488868616055697)\n",
      "(408, 0.0001645406155148521)\n",
      "(409, 0.0001547735300846398)\n",
      "(410, 0.000145573983900249)\n",
      "(411, 0.00013688464241568)\n",
      "(412, 0.00012870115460827947)\n",
      "(413, 0.00012100925232516602)\n",
      "(414, 0.00011375657049939036)\n",
      "(415, 0.00010690958879422396)\n",
      "(416, 0.00010047837713500485)\n",
      "(417, 9.442339069209993e-05)\n",
      "(418, 8.869833982316777e-05)\n",
      "(419, 8.333902951562777e-05)\n",
      "(420, 7.82841962063685e-05)\n",
      "(421, 7.351234671659768e-05)\n",
      "(422, 6.903507164679468e-05)\n",
      "(423, 6.481824675574899e-05)\n",
      "(424, 6.085859058657661e-05)\n",
      "(425, 5.711619451176375e-05)\n",
      "(426, 5.360981231206097e-05)\n",
      "(427, 5.0309750804444775e-05)\n",
      "(428, 4.7206642193486914e-05)\n",
      "(429, 4.4282547605689615e-05)\n",
      "(430, 4.1534280171617866e-05)\n",
      "(431, 3.896511407219805e-05)\n",
      "(432, 3.654066313174553e-05)\n",
      "(433, 3.4252210753038526e-05)\n",
      "(434, 3.2118499802891165e-05)\n",
      "(435, 3.0110493753454648e-05)\n",
      "(436, 2.8220258172950707e-05)\n",
      "(437, 2.6440451620146632e-05)\n",
      "(438, 2.47744992520893e-05)\n",
      "(439, 2.3213047825265676e-05)\n",
      "(440, 2.1741849195677787e-05)\n",
      "(441, 2.036358455370646e-05)\n",
      "(442, 1.9068707842961885e-05)\n",
      "(443, 1.785571475920733e-05)\n",
      "(444, 1.6718349797884002e-05)\n",
      "(445, 1.5644123777747154e-05)\n",
      "(446, 1.4644419934484176e-05)\n",
      "(447, 1.370727568428265e-05)\n",
      "(448, 1.2820925803680439e-05)\n",
      "(449, 1.1996942703262903e-05)\n",
      "(450, 1.1224000445508864e-05)\n",
      "(451, 1.049367983796401e-05)\n",
      "(452, 9.812955795496237e-06)\n",
      "(453, 9.177262654702645e-06)\n",
      "(454, 8.581353540648706e-06)\n",
      "(455, 8.022465408430435e-06)\n",
      "(456, 7.494390047213528e-06)\n",
      "(457, 7.004325198067818e-06)\n",
      "(458, 6.542595656355843e-06)\n",
      "(459, 6.112522896728478e-06)\n",
      "(460, 5.70796419196995e-06)\n",
      "(461, 5.333377885108348e-06)\n",
      "(462, 4.977057869837154e-06)\n",
      "(463, 4.649547008739319e-06)\n",
      "(464, 4.337812697485788e-06)\n",
      "(465, 4.0490504034096375e-06)\n",
      "(466, 3.778091922868043e-06)\n",
      "(467, 3.525851525409962e-06)\n",
      "(468, 3.287941581220366e-06)\n",
      "(469, 3.0673072615172714e-06)\n",
      "(470, 2.8612582809728337e-06)\n",
      "(471, 2.667991338967113e-06)\n",
      "(472, 2.486660150680109e-06)\n",
      "(473, 2.317302687515621e-06)\n",
      "(474, 2.1599580577458255e-06)\n",
      "(475, 2.013990069826832e-06)\n",
      "(476, 1.8759985778160626e-06)\n",
      "(477, 1.748239583321265e-06)\n",
      "(478, 1.6277060694847023e-06)\n",
      "(479, 1.515402573204483e-06)\n",
      "(480, 1.4107762353887665e-06)\n",
      "(481, 1.314650830863684e-06)\n",
      "(482, 1.2232476365170442e-06)\n",
      "(483, 1.138626544161525e-06)\n",
      "(484, 1.0591884347377345e-06)\n",
      "(485, 9.86061422736384e-07)\n",
      "(486, 9.168962264993752e-07)\n",
      "(487, 8.529309525329154e-07)\n",
      "(488, 7.931241725600557e-07)\n",
      "(489, 7.373946004918253e-07)\n",
      "(490, 6.856977279312559e-07)\n",
      "(491, 6.377973704729811e-07)\n",
      "(492, 5.926303856540471e-07)\n",
      "(493, 5.499422854882141e-07)\n",
      "(494, 5.119704269418435e-07)\n",
      "(495, 4.747224693346652e-07)\n",
      "(496, 4.4071094862374594e-07)\n",
      "(497, 4.094659118436539e-07)\n",
      "(498, 3.7954026765874005e-07)\n",
      "(499, 3.531332026796008e-07)\n"
     ]
    }
   ],
   "source": [
    "N, D_in, D_out = 64, 1000, 10\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, D_out),\n",
    "\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = torch.optim.SGD\n",
    "adadelta = torch.optim.Adadelta\n",
    "adagrad = torch.optim.Adagrad\n",
    "rmsprop = torch.optim.RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.2541\n",
       " 0.0638\n",
       "-0.1823\n",
       "   ⋮   \n",
       " 2.9278\n",
       " 0.8388\n",
       " 0.6463\n",
       "[torch.FloatTensor of size 1000]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
